{"Title": "Practical Conditional Neural Process Via Tractable Dependent Predictions", "Authors": ["Stratis Markou", "James Requeima", "Wessel Bruinsma", "Anna Vaughan", "Richard E Turner"], "Abstract": "Conditional Neural Processes (CNPs; Garnelo et al., 2018a) are meta-learning models which leverage the flexibility of deep learning to produce well-calibrated predictions and naturally handle off-the-grid and missing data. CNPs scale to large datasets and train with ease. Due to these features, CNPs appear well-suited to tasks from environmental sciences or healthcare. Unfortunately, CNPs do not produce correlated predictions, making them fundamentally inappropriate for many estimation and decision making tasks. Predicting heat waves or floods, for example, requires modelling dependencies in temperature or precipitation over time and space. Existing approaches which model output dependencies, such as Neural Processes (NPs; Garnelo et al., 2018b) or the FullConvGNP (Bruinsma et al., 2021), are either complicated to train or prohibitively expensive. What is needed is an approach which provides dependent predictions, but is simple to train and computationally tractable. In this work, we present a new class of Neural Process models that make correlated predictions and support exact maximum likelihood training that is simple and scalable. We extend the proposed models by using invertible output transformations, to capture non-Gaussian output distributions. Our models can be used in downstream estimation tasks which require dependent function samples. By accounting for output dependencies, our models show improved predictive performance on a range of experiments with synthetic and real data.", "ID": "3pugbNqOh5m", "URL": "https://openreview.net/forum?id=3pugbNqOh5m", "Review": [{"correctness": "4: All of the claims and statements are well-supported and correct.", "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.", "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "flag_for_ethics_review": ["NO."], "recommendation": "8: accept, good paper", "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.", "review": "The authors present a class of neural process models that are able to produce correlated predictions while amenable to exact, simple and scalable maximum likelihood optimization supporting multiple outputs. By using invertible transformations (gaussian copula), the model is able to capture non-Gaussian output distributions. Experiments with artificial and real data (EEG and climate), highlight the predictive ability of the proposed model. The authors introduce a relatively straightforward extension for Gaussian neural processes in which both mean and (half of the) covariance functions are specified as neural networks, and the covariance function is either explicitly calculated as an inner product in (7) or as a squared exponential covariance function modulated in magnitude by an auxiliary neural network and calculated using the outputs of a neural network rather than the input data itself (x_t, x_c, y_c). Their multi-output and non-Gaussian strategy follows the modulated kernel and Gaussian copula formulation, respectively. Comprehensive experiments on both artificial and real data demonstrate the advantages of the proposed model over the related, but more computationally expensive, fullConvGNP model. Moreover, on real data, the proposed model outperforms both mean field, convNP and MOGP approximations.\n\nSomething that is not discussed in the paper is the setting of the length scale of the squared exponential kernel.\n\nConsidering that one of the motivations of the proposed approach is how prohibitive existing approaches are, having estimates of computational cost and/or runtime experiments could be a welcome addition to the paper. The proposed approach though technically simple relative to existing literature in neural and Gaussian process literature it is well motivated, technically sound and with comprehensive experimental results that support the improved predictive performance claimed by the authors. "}, {"correctness": "4: All of the claims and statements are well-supported and correct.", "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "flag_for_ethics_review": ["NO."], "recommendation": "6: marginally above the acceptance threshold", "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.", "review": "There is a long line of recent interesting work on neural processes, a scalable and more flexible alternative to GPs for performing prediction at a set of test points (x1, ..., xm) given a conditioning set ((x, y)_1, ..., (x, y)_n). This mapping is learned via meta-learning. \n\nThis paper addresses a core issue of the popular conditional neural process: the predictions at each test point are conditionally independent given the conditioning set. This is an inappropriate modeling assumption for many real-world datasets. In response, the authors propose to go beyond a non-diagonal Gaussian to describe the joint distribution. For example, they use some structured Gaussian covariances (linear, kvv) and also a Gaussian copula model. \n\nThey demonstrate both qualitatively and quantitatively that modeling these dependencies improves the performance of the model on a variety of datasets spanning application domains.\n\n\n \nWhen I saw 'Tractable Dependent Predictions' in the title, I assumed that a normalizing flow was being used to capture the joint distribution. This is a modern, flexible family of density estimators for which computing the likelihood is tractable. I was disappointed when I found that these weren't used in the paper. The copula model is very close to a full normalizing flow model. Can you explain what would be necessary to extend your method to a full normalizing flow?\n\nI found the discussion of fullconvgp inadequate, as it seems like the most relevant baseline for capturing these dependencies. It would be very helpful if you included an equation or two. I shouldn't need to dig into the literature to understand the difference between your method and prior work. What exactly is the difference between fullconvgp and convgnp?\n\nI was disappointed that there were no error bars. What sources of variance are you averaging over?\n\nCan you make the analysis of fig 3 more quantitative? Right now, the assertion that the samples are better is too qualitative. Can you check, for example, a q-q plot at a few x axis locations?\n\nI'm curious how your model behaves when the data truly has diagonal covariance. I expect that there are off-diagonal artifacts due to overfitting. Can you run a quick experiment to check this?\n\n\n\n I feel that the contribution is fairly incremental and I'm disappointed that they did not consider full normalizing flow models. However, I found the exposition engaging and the experiments thorough. "}, {"correctness": "4: All of the claims and statements are well-supported and correct.", "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.", "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.", "flag_for_ethics_review": ["NO."], "recommendation": "6: marginally above the acceptance threshold", "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.", "review": "The manuscript proposes variants of Neural process (NP), which can model correlation in the input (and in the output for multiouput regression). The main idea is to directly parameterize the mean and the covariance functions of a Gaussian predictive via neural networks. The authors also propose to use Copulae to handle non-Gaussian marginal distributions.   Strengths \n1. The proposed methods are easy to understand and implement. \n2.  The way of modeling dependencies in the input (or the output) through a covariance function looks simple but indeed improves the performance.\n3. Using Copulae, the proposed methods can handle non-Gaussian distributions\n\nWeaknesses \n1. I think the novelty of the proposed methods is not striking. The proposed method looks like a direct implementation of a Gaussian process with deep kernel learning in the meta learning setting (through the form of NP): just including the global representation vector \\mathbb{r} into the mean and the covariance functions for a GP.    \n\n2. The descriptions (in Introduction) related the reference [Bruinsma et al., 2021] looks a little bit confusing. First, the name \u201cGaussian Neural Process\u201d (and the concept of modeling a Gaussian predictive using neural networks) is already introduced in the reference paper. Second, it sounds that the proposed methods are belonging to the same class as the FullConvGNP [Bruinsma et al., 2021] is (due to the same class name \u201cGaussian Neural Process\u201d). Are the proposed methods also translation equivariant as the FullConvGNP is? If this is true, it is required to include proofs. If not, I think the introduction section should be carefully revised to clearly differentiate the proposed methods from the FullConvGNP [Bruinsma et al., 2021].      \n I like that fact that the authors include multiple data sets that can show the effectiveness of modeling dependencies in the input (or the output). However, I think the novelty of the proposed methods is not that impressive. \n\nMinor comments\n\nIn the end of page 1: \u201cCNPs model their respective outputs y_m and y_m\u2019 independently, that is y_m \\prep y_m\u2019\u201d. It would be better if the mathematical definition of the symbol \u201c\\prep\u201d was included in the manuscript. \n\nThe current version of the manuscript is not printable (on Windows 10). I tried to print the manuscript out from Adobe reader and Chrome several times but failed to do it. \n "}], "Decision": "Accept (Poster)", "Metareview": "This paper proposes a class of neural processes that lifts the limitations of conditional neural processes (CNPs) and produces dependent/correlated outputs but that, as CNPs, is inherently scalable and it is easy to train via maximum likelihood. The proposed model is extended to multi-output regression and to capture non-Gaussian output distributions. Results are presented on synthetic data, an electroencephalogram dataset and on a climate modeling problem. The paper parameterizes the prediction map as a Gaussian, where the mean and covariance are determined using neural networks. Non-Gaussian prediction maps are obtained using copulas. \n\nTechnically speaking, the reviewers found the approach to be incremental and only marginally significant and I agree with them. Issues such as estimates of computational cost, using fixed lengthscales for the covariances and relationships/using normalizing flows have been addressed by the authors satisfactorily. Empirically, the contribution of the paper is somewhat significant, as it provides similar flexibility to other more computationally expensive processes and more general assumptions than conditional neural processes.", "Venue": "ICLR-2022"}
{"Title": "When should agents explore?", "Authors": ["Miruna Pislar", "David Szepesvari", "Georg Ostrovski", "Diana L Borsa", "Tom Schaul"], "Abstract": "Exploration remains a central challenge for reinforcement learning (RL). Virtually all existing methods share the feature of a *monolithic* behaviour policy that changes only gradually (at best). In contrast, the exploratory behaviours of animals and humans exhibit a rich diversity, namely including forms of *switching* between modes. This paper presents an initial study of mode-switching, non-monolithic exploration for RL. We investigate different modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. We also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. Finally, we report a promising initial study on Atari, using two-mode exploration and switching at sub-episodic time-scales.", "ID": "dEwfxt14bca", "URL": "https://openreview.net/forum?id=dEwfxt14bca", "Review": [{"correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.", "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "flag_for_ethics_review": ["NO."], "recommendation": "6: marginally above the acceptance threshold", "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.", "review": "This paper proposes to study exploration at different levels of granularity. Current methods either explore at the level of individual steps (e.g., \\epsilon-greedy), or at the level of experiments (e.g. first a reward-free exploration phase, followed by a task-dependent learning phase using the gathered data). This paper proposes to study exploration at the intra-episodic level, i.e. where the agent switches between exploration and exploitation within the same episode. \n\nThey discuss various design choices to perform exploration at this level, for example switching after a certain number of steps or with a certain probability, or switching based on the discrepancy between the predicted value and actual experienced value. \n\nThe experimental results show that including intra-episodic exploration gives a modest benefit over other exploration schemes when using an R2D2 base agent. Other insights are also included, which show that the proportion of exploration does change throughout the learning process, indicating that different degrees of exploration are useful at different stages. They also show that the informed switching component learns switching behaviors which are non-uniform throughout the episodes. Strengths:\n- this paper investigates a novel area, which seems very important at a high level.\n- it also proposes novel methods to start to address this area.\n\nWeaknesses:\n- the paper does not feel very focused. There are lots of different ideas and methods presented, but the takeaways are unclear. \n- the figures are a bit hard to parse \n\nFirst, I applaud the authors for tacking a new and unexplored area. However, the takeaways and next research steps are unclear. At least on Atari, the benefits of intra-episodic exploration seem limited. \n\nSome suggestions for improving the paper:\n- It would be helpful to have the different algorithm variants in Section 2 spelled out, especially regarding the different switching mechanisms. The current descriptions are quite high-level and it would be helpful to make them concrete.\n- The tuple notation (shown in Figure 2) is a bit hard to parse. It would be helpful to show several examples, so that the differences in the different elements of the list can be seen more clearly.\n- I think the paper would be a lot stronger if there were some tasks which more convincingly demonstrated the benefits of intra-episode exploration. This could be a new task which the authors design themselves. I think including the Atari experiments is useful in that it shows their methods can improve performance on a standard benchmark, however, the monolithic exploration methods already work quite well on these tasks, and it is not clear if there is that much more improvement to be had by exploring at a finer granularity. I do agree with the authors that in the \u201cbig picture\u201d, monolithic exploration is likely suboptimal and more informed exploration will be necessary. I found their motivating example of learning to ride a bike while maintaining necessary daily activities to be very helpful. Can you design some task which distills this task in a simpler format? For example, some setup where the agent must regularly find food from a predictable source (exploiting) but also must explore when it can between finding food. Introducing new tasks which measure the ability to optimally switch between explore and exploit modes would also be useful to the community in building on this work. \n\n I'm on the fence about accepting this paper. On one hand I think it is good that the authors are exploring a new and important area and the ideas are interesting. On the other hand, this work still feels preliminary and the benefits of intra-episode exploration are not yet convincingly demonstrated.  I am not strongly opposed to accepting this paper since it could at least be a starting point for research in this area. However I think that if the authors could introduce new tasks where intra-episodic exploration convincingly helps, then I think this would be a very strong submission to a later conference. "}, {"correctness": "4: All of the claims and statements are well-supported and correct.", "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "flag_for_ethics_review": ["NO."], "recommendation": "8: accept, good paper", "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.", "review": "This paper studies switching between exploit and explore modes in reinforcement learning. It discusses switching mechanisms based on time (\"blind switching\") and based on state (\"informed switching\"). Studying seven Atari games, an empirical analysis of different switching mechanisms is performed. Overall, the paper is very strong, well-motivated, and empirically sound. Most of my concerns are with clarifying details of the experiments and improving the exposition. These are listed below:\n\n- paragraph 2 of the introduction: the typical answer to \"when to explore\" is when the agent is unfamiliar with the environment or its structure, i.e., early in its interactions with the environment. I would highlight why the heuristic of \"explore earlier\" should be challenged here instead.\n\n- paragraph 3 of the introduction: it's not clear what the connection to schizophrenia here is, other than that this was a study of explore vs. exploit; are these individuals somehow impaired in choosing between these options, etc.?\n\n- paragraph 1 of section 2 (methods): not sure that the example of riding a bicycle works - is the targeted acquisition of a new skill really exploration?\n\n- section 2.1, description of \"episode-level\": it would be more useful to define \"episode\" in the context studied here, rather than using the example of training games vs. tournament matches.\n\n- caption to Figure 1: the differences for D-G are unclear just by looking at this figure and the caption, other than that they are different intra-episode approaches; it would be better to clarify this\n\n- section 2.2, description of \"blind switching\": this refers to \"fractional episode length\", but if we don't get to choose the length of an episode, how do we implement fractional episode length ex-ante?\n\n- section 2.3, description of \"bandit adaptation\": the citations should be in-text citations\n\n- section 3: it would be useful to explain the observed differences between these domains; are there hypotheses for why certain domains are better suited for different switching mechanisms? The discussion in Section 3.3 starts to get at this, but does not specifically address why the differences may arise.\n\n- section 3.2: in appendix A.3, the compute budget is mentioned as being 2B frames; please clarify this difference\n\n- section A.1, regarding using the full action sets: does this mean that some games have meaningless actions, i.e., actions that cause no effect in the world? how does this affect learning and why was this choice made?\n\n- section A.1, regarding no life-loss signal: what exactly is an episode here if there is no life-loss signal? Is it just 108,000 frames (and why this number?) How does scoring work?\n\n- section A.1: why was the choice to use raw, unprocessed frames made? in particular, what is the effect of keeping color information for learning?\n\n- section A.2: is 1 TPU used (as mentioned here) or 2 TPUs (as mentioned in Section A.3)?\n\n- section A.2: citations should be in-text citations; \",\" should be \";\" after Schaul et al. (2021) in paragraph 2, line 4.\n\n- section A.3: \"while 2 TPUs\" should be \"using 2 TPUs\".\n\n- The captions to Figures 9 - 15, especially 9 and 10, should be expanded to clarify what the reader can learn from this figure (e.g., any hypotheses on what accounts for the differences between different environments) The paper studies the relatively under-explored question of when agents should explore, introduces a novel exploration trigger called \"value promise discrepancy\", and performs a thorough empirical analysis in the domain of seven Atari games. Please see the main review for detailed comments and suggestions for improvement. "}, {"correctness": "4: All of the claims and statements are well-supported and correct.", "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "flag_for_ethics_review": ["NO."], "details_of_ethics_concerns": "Not applicable.", "recommendation": "8: accept, good paper", "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.", "review": "This paper investigates when to switch between exploitation and exploration and how long to stay in each exploration mode during RL learning. It proposes new ways to explore the subject, especially with intra-episodic exploration variants. It presents a large body of study results (10 pages of appendices!!), and concludes with very thought-provoking suggestions and discussions. This paper conducts a series of experiments aimed at answering the question of when we should switch between exploitation and exploration during RL learning.\nAs positive points we can cite the wide related bibliography, from analogies with the animal system (human included) of exploring to other techniques such as the use of options. Another point to emphasize is the clear, careful and pleasurable writing that the authors developed in the paper. The authors managed to transmit a moment of reflection and expansion of thoughts about the possible behaviors shown by agents who learn with RL. Moments of reflection in an area that transforms at a very high speed are always welcome.\n\nPerhaps the negative point is the limitation of the contribution -- since it is still a study that must be deepened in order to provide effective and efficient guidelines for RL system developers working in real applications.\n\nHowever, it is an in-depth study, with interesting results. The article is worth being divulged to remind everyone working in the RL area that there is still a lot to study, investigate and evaluate in order to have robust, efficient and effective systems. The paper brings an in-depth study, with interesting results and very well written. It is worth being divulged to remind everyone who works in the RL area that there is still a lot to study, investigate and evaluate in order to have robust, efficient and effective systems. "}, {"correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.", "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.", "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.", "flag_for_ethics_review": ["NO."], "recommendation": "6: marginally above the acceptance threshold", "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.", "review": "This paper proposes a mode-switching strategy for the exploration/exploitation dilemma instead of monolithic behaviour policies in order to obtain more diverse behaviour. Different granularities for the timing of the switches as well as different switching mechanisms are investigated (blind vs. informed switching). The focus for exploration is not on how, but when. For exploration, they both use Random Network Distillation (RND) as well as a uniform policy. Their experiments are conducted on the Atari Learning Environment (ALE), where they provide performance and diversity results. The idea of mode-switching is interesting. However, the presentation of the motivation as well as the results seem somewhat weak in my view. The main comparison baseline for the mode-switching architecture is the monolithic variant, where typically sparse rewards in hard exploration tasks are augmented with intrinsic reward signals. This means that the modes that they switch between in this paper are merged homogeneously in time. A valid problem that the authors point out for the monolithic case is that the scale of the intrinsic reward signal has to be tuned and may need to change in time. But there is no comparison to these methods and the superiority of their method to the monolithic variants are not highlighted well enough.\n\nThe authors try to circumvent the performance comparisons with other baselines by saying that they can obtain more diverse behaviours (in terms of exploration strategies?) and say that they don\u2019t aim to show improved performance. However, this diversity argumentation is not strong enough in my view. First, it is not clear to me if the authors want to focus on the diversity of behaviour that is obtained within one variant of the mode-switching, e.g. informed switching, or whether they want to highlight the diversity across different granularities and switching mechanisms. If it is the latter, the diverse behaviour doesn\u2019t necessarily translate into performance for the different games. And best performance for different games might be obtained with different strategies, but since this strategy is fixed prior to the experiment, it is not clear to me how this helps the diversity argument of the method itself. The authors point out that Montezuma\u2019s Revenge and Phoenix have their best performance with different mode-switching behaviours, but these results have a really high variance. Perhaps, 3 seeds are just not enough and more seeds are needed to draw reliable conclusions. I believe that the idea presented in this paper is interesting but the results are lacking. The related work section briefly covers some similar methods, and I think comparisons are still needed with these other methods. E.g. GoExplore also focuses on the *when* question of exploration and should be included as a baseline as well as works with monolithic behaviour policies where the mode-switching is replaced by a weighting problem of external and intrinsic rewards of a single behaviour policy. "}], "Decision": "Accept (Spotlight)", "Metareview": "Exploration can happen at various levels of granularity and at different times during an episode,  and this work performs a study of the problem of exploration (when to explore/when to switch between exploring and exploitation, at what time-scale to do so, and what signals would be good triggers to switch). The study is performed on atari games.\n\nStrenghts:\n------------\nThe study is well motivated and the manuscript is overall well written\nStudies a new problem area, and proposes an initial novel method for this problem\nextensive study on atari problems\n\nWeaknesses\n--------------\nsome clarity issues as pointed out by the reviewers\nno illustrative task is given to give a more intuitive exposition of the \"when to explore\" problem\ncomparison to some extra baselines like GoExplore would have been insightful\n\nRebuttal:\n----------\nMost clarity issues have been addressed satisfactorily. It has been explained why some requests for extra baselines would be challenging/or not relevant enough. While the authors agree that GoExplore would be an interesting baseline, they seem to have not added it. An illustrative task was not provided.\n\nSummary:\n------------\nAll reviewers agree that this manuscript opens up and tackles a novel direction in exploration, and provides an extensive empirical study on atari games (a standard benchmark for such problem settings). While I agree with the reviewers that point out that this paper could have been made stronger by adding an illustrative task and additional baselines like GoExplore, there is a general consensus that the provided empirical study on this novel problem setting is a good contribution in itself. Because of this I recommend accept.", "Venue": "ICLR-2022"}
{"Title": "Iterative Graph Self-Distillation", "Authors": ["Hanlin Zhang", "Shuai Lin", "Weiyang Liu", "Pan Zhou", "Jian Tang", "Xiaodan Liang", "Eric Xing"], "Abstract": "How to discriminatively vectorize graphs is a fundamental challenge that attracts increasing attentions in recent years. Motivated by the recent success of unsupervised contrastive learning, we aim to learn graph-level representation in an unsupervised manner. Specifically, we propose a novel unsupervised graph learning paradigm called Iterative Graph Self-Distillation (IGSD) which iteratively performs the teacher-student distillation with graph augmentations. Different from conventional knowledge distillation, IGSD constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and unsupervised contrastive loss. Finally, we show that finetuning the IGSD-trained models with self-training can further improve the graph representation power. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.", "Review": [{"title": "Motivation of key elements and significance of the idea are unclear", "review": "Summary:\n\nThis paper proposes a self-distillation based graph augmentation mechanism to alleviate the drawbacks of existing MI based models w.r.t. their high dependency towards negative sampling. Quantitatively the proposed model achieves encouraging results. However it would have been better if the system designs and significant difference of IGSD from existing work are discussed.\n\n\nStrength:\n\n- This work has clearly discussed a drawback of existing unsupervised MI based models which is the leading approach in graph classification\n- They propose a mechanism to address this issue with satisfiable quantitative results on unsupervised setting and extended semi-supervised setting with self-training also supported quantitatively.\n- Paper is clear in general, with a clear research problem, proposes mechanism for unsupervised/semi-supervised graph representation domain and encouraging quantitative results.\n\nWeakness:\n\n-There is a lack of qualitative analysis and discussion of the proposed method. \n-In Section 4.3 \"Performance with different amount of negative pairs\", it is not clear the reasoning of the provided observation from Figure 3a.\n-It is not clear the motivation behind selecting a teacher-student network for obtaining different views of the graph. These networks are normally used for knowledge transfer, but here used for contrastive learning. How is this more beneficial than an ensemble model w/o knowledge transfer step of Eq. 3. \n-The core difference of IGSD from CMC-graph is that CMC uses MI based contrastive between local patch representation and graph rep, while IGSD uses L2 based contrastive between 2 graph representations. Input, Encoders and projections are the same for both architectures. It could be useful to add some analysis to discuss these differences and their contributions to clearly understand the significance of IGSD.\n-This paper seems to have state-of-the-art results (although it is based on graph kernel). Why the results are not included?\n\nConvolutional Kernel Networks for Graph-Structured Data, ICML-2020\n\n\n\n=======================\n\nafter rebuttal:\n\n\nI thank the authors for the response. I still have concerns re their comparison with GCKN (ICML-2020). The reproduced results by the authors are different significantly from the published one in Table 1 of GCKN paper (~5%). E.g. MUTAG is 92.8 in original paper, but the authors report 87.2 for GCKN. The difference is significant.\n\nTherefore, I will keep my original rating.\n \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, {"title": "An interesting paper", "review": "Overall Comments:\nLearning graph-level representations with only labels has been explored by many works. However, it's not easy to annotate every graph. This paper applies the ideas from semi-supervised classification task to improve the representation quality learned by graph neural network. Specifically the proposed solution combines several kinds of existing techniques including diffusion graph augmentation, mean teacher consistency, debiased contrastive loss and pseudo class consistency. Finally they are combined together to act as a regularization term by utilizing the unlabelled data. From this point of view, the novelty of this work is incremental, but it's still an interesting work for improving graph-level representations.\n\nClarity:\nThe presentation is not clear enough. There exists many claims that are not clear, shown as follows:\n\n1. In the last sentence of 3rd paragraph in introduction section, it's difficult to get the connection between negative samples mining and self-distillation strategy. Why using the self-distillation can alleviate the dependency on negative samples mining? The unsupervised objective in Equation 4 still depends on negative samples. \n2. In Section 2.1, the notation for augmentations. Why are the graphs G_L attached without labels after being augmented?\n3. In Section 2.3, authors firstly use PPR to augment node features, then randomly remove edges to create a corrupted graphs. According to the description, the question is how many views that will be used in follow sections? I guess that the graph feature from original graph will be fed to student network, and the augmented corrupted graph will be fed to teacher network.\n\nQuestions for Rebuttal:\n1. Please clarify the mentioned questions above.\n\n2. The proposed method contains an encoder, projector, and predictor. The question is why we need a projector g to get a higher dimension z? Does it have a big influence on the performance? Could you please give the complete definition of function g and h?\n\n3. The definition of L^{con}  in Equation 2 is for positive sample extracted from the same graph G_i. However, the complete unsupervised loss needs negative samples G_j. Could you please also give the definition for L^{G_i, G_j}?\n\n4. The overall loss consists of supervised and unsupervised loss. The L^{sup} has conflict to the first term in Equation 7. Both of them use labels but it's difficult to tell which one should be aligned with the supervised loss shown in the ablation study (Table 2). The SupCon has never been shown in the main content before. Please pay attention to make it clear.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"title": "Experiments need to be improved", "review": "------------------------------------\n\nupdate after reading the authors' response.\n\nThe authors didn't address my question \"Did the authors perform a significance study?\" A significance test such as double-sided t-test is needed to verifying whether the proposed method is significantly better than baselines.\n\n-------------------------------\n\nThis paper proposed a distillation approach for unsupervised graph representation learning. The approach partially builds upon contrastive self-supervised learning which contrasts pairs of augmented graphs. The approach is extended to the semi-supervised setting. The authors performed evaluation in graph classification and regression tasks. \n\nI recommend to reject this paper, due to the following major concerns: 1) experimental results are not strong; 2) important baselines were not compared; 3) important details such as optimal hyperparameter values are missing. \n\nMy major concerns of this paper include:\n1. The improvement of the proposed approach over baselines seem not significant. For example, in Table 1, comparing the mean and standard deviation of the proposed approach and CMC-GRAPH, it seems that the difference is not statistically significant. Did the authors perform a significance study?\n2. In the experiments, why the authors didn't compare with GCC, which is a contrastive self-supervised learning approach applied to graph classification?\n3. There are many other unsupervised graph representation learning methods. The authors need to compare with more to substantiate this work.\n4. In hyperparameter tuning, the authors gave the range of hyperparameters tuned, but didn't give the optimal value of the hyperparameters, which make the paper difficult to reproduce.\n5. In table 1, the authors excluded some results since they need more than 1 day to obtain. It is common for deep learning models to run several days to obtain results. I don't think it is proper to exclude these results simply because the runtime is more than 24 hours.\n\nHowever, the paper does have a few strong points.\n1. The ablation studies are well designed and the results are insightful.\n\n2. The paper is well-written and easy to follow, with a clear organization.\n\n3. The experiments were conducted on a rich collection of datasets. \n\nOther comments.\n1. In equation (3), the authors can draw a connection with MoCo.\n2. In Table, why didn't report mean and standard deviation of the results?\n3. For this result \"When batch size is\ngreater than 32, IGSD outperforms CMC-Graph and the performance gap becomes larger as the batch\nsize increases.\",  can the authors provide a reason that can possibly explain this phenomenon?\n4. The authors can add some statistics of the datasets used in Figure 2.\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"title": "Official Blind Review #3", "review": "This paper proposed a method for learning graph-level representation in an unsupervised contrastive way. Instead of contrasting between graph-level representation and patch representation like InfoGraph [1], they contrast graph-level representation of a graph to its augmented variation using a teacher-student framework.\n\n* why use InfoNCE objective instead of the Jensen-Shannon mutual information objective used in InfoGraph [1] ? \n\n* The major concern about this paer is that the proposed method encourages the closeness of augmented views from the same graph instances but provide no guarantee that the transformation used (graph diffusion and sparsification with PPR + random remove edges in this paper) would be label-preserving. For example, in molecular datasets, if we drop an edge and that edge happens to be in a structural motif, it will drastically change the attributes/labels of the molecule. \n\n* $v$ represents nodes in section 2.1 and 2.2 and it represents graph instances in section 3.1 and Figure 1. This can be confusing I suggest changing the notation in section 3.1 and Figure 1.1 to $G$\n\n[1] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. arXiv preprint arXiv:1908.01000, 2019.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}], "Decision": "Reject", "Metareview": "This paper proposes an unsupervised graph learning method [Iterative Graph Self-Distillation (IGSD)] by iteratively performing self-distillation to contrast graph pairs under different augmented views. This idea is then extended to semi-supervised setting where via a supervised contrastive loss and self-training. The method is empirically evaluated on some semi-supervised graph classification and molecular property prediction tasks, and has achieved promising results.\n\nReviewers agree that the method is interesting and the paper is well-written. The biggest concern from reviewers related to experimental evaluations of the method. The authors responded to this and included additional experiments. Although the reviewers appreciate the provided results and explanations, at the end they were not convinced about the empirical assessments. In particular, R1's post rebuttal comment indicates concerns about the reported performance of GCKN, which is different from the published one in Table 1 of GCKN paper. I encourage authors to improve on these experimental discrepancies and resubmit. ", "URL": "https://openreview.net/forum?id=Z532uNJyG5y", "ID": "Z532uNJyG5y", "Venue": "ICLR-2021"}
{"Title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "Authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "Abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "Review": [{"confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "The paper has intriguing ideas but requires more work"}, {"title": "An interesting approach", "review": "The paper studies how the vulnerability of a neural network model depends on its input dimension. The authors prove that for an *untrained* model, randomly initialized with Xavier initialization, the gradient of the loss wrt the input is essentially independent of the architecture and task. This implies that the major factor affecting the norm of that gradient is the input dimension. They then support their argument by experiments measuring the relation between adversarial vulnerability and gradient norm using various *trained* models (including adversarially regularized ones).\n\nI find the main theoretical result interesting. While this is a known fact for the simple case of linear classifiers, extending it to arbitrarily deep networks is a valuable contribution. The proof crucially relies on properties of the specific initialization scheme to show that the gradient does not change too much during backproparagation through the layers. The most significant limitation of the result (which the authors kindly acknowledge) is that this result only holds at initialization. Hence it cannot distinguish between different training methods or between how different architectures evolve during training. Since the situation in adversarial robustness is much more nuanced, I am skeptical about the significance of such statements.\n\nOn the experimental side, the finding that gradient regularization improves adversarial robustness to small epsilon values has been made multiple times in the past (as the authors cite in the related work section). It is worth noting that the epsilon considered is 0.005 in L_inf (1.275/255) which is pretty small. This value corresponds to the \"small-epsilon regime\" where the behavior of the model is fairly linear around the original inputs and thus defenses such as FGSM-training and gradient regularization are effective.\n\nThe authors also perform an interesting experiment where they train models on downsampled ImageNet datasets and find that indeed larger input dimension leads to more vulnerable models.\n\nWhile I find the results interesting, I do not see clear implications. The fact that the vulnerability of a classifier depends on the L1 norm of the input gradient is already known for any locally linear classifier (i.e. deep models too), and it is fairly clear that the L1 norm will have a dimension dependence. The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion. Given that the experimental results are also not particularly novel, I recommend rejection.\n\n[UPDATE]: Given the overall discussion and paper updates, I consider the current version of the paper (marginally) crossing the ICLR bar. I update my score from a 5 to a 6.\n\nMinor comments to the authors:\n-- I think || x ||_* is more clear than |||x||| for the dual norm.\n-- Consider using lambda for the regularization, epsilon is confusing since it is overloaded.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"title": "Considered question seems poorly motivated, significance of analysis and conclusions yet to be demonstrated", "review": "This paper argues that adversarial vulnerability of neural networks increases with input dimension. Theoretical and empirical evidence are given which connect the l_p norm of the gradient of the training objective with the existence of small-worst case l_q perturbations. This connection is made by assuming that the learned function is well approximated by a linear function local to the sampled input x. By making assumptions on the initialization scheme for some simple architectures, the authors show that the l_p norm of the gradient for randomly initialized network will be large, and provide empirical evidence that these assumptions hold after training. These assumptions imply bounds on the typical magnitude of the gradient of the loss with respect to a single input coordinate, this then implies that the overall gradient norm will depend on the input dimension.\n\nI found this paper well written. The mathematical assumptions are presented in a clear, easy to understand manner. Also high level intuition is given around their main theorems which help the reader understand the main ideas. However, I have a  number of concerns about this work.\n\nThe first is, I do not buy the motivation for studying the \"phenomenon\" of small worst-case l_p perturbations. I realize this statement applies to a large body of literature, but since the publication of  [1] we are still lacking concrete motivating scenarios for the l_p action space. I would encourage the authors instead to ask the closely related but more general question of how we can improve model generalization outside the natural distribution of images, such as generalization in the presence of commonly occurring image corruptions [2]. It's possible that the analysis in this work could better our understanding model generalization in the presence of different image corruptions, indeed by making similar linearity assumptions as considered in this work, test error in additive Gaussian noise can be linked with distance to the decision boundary [3,4]. However, this particular question was not explored in this work.\n\nSecond, the work is one of many to relate the norm of the gradient with adversarial robustness (for example, this has been proposed as a defense mechanism in [5,6]). I also suspect that the main theorem relating gradient norm to initialization should easily follow for more general settings using the mean field theory developed by [7,8] (this would be particularly useful for removing assumption H1, which assumes the ReLU activation is a random variable independent of the weights). Overall, I don't see how gradient norms explain why statistical classifiers make mistakes, particularly for more realistic attacker action spaces [9]. Even for \"small\" l_p adversarial examples there seem to be limitations as to how much gradient norms can explain the phenomenon --- for example even max margin classifiers such as SVM's have \"adversarial examples\". Furthermore, adversarial training has been shown to reach a point where the model is \"robust\" locally to training points but this robustness does not generalize to the points in the test set [10]. In fact, for the synthetic data distributions considered in [10], it's proven that no learning algorithm can achieve robustness given insufficient training data.\n\nFinally, the main conclusion of this work \"adversarial vulnerability of neural networks increases with input dimension\" is an overly general statement which needs a much more nuanced view. While experiments shown in [11] support this conclusion for naturally trained networks, it is shown that when adversarial training is applied the model is more robust when the input dimension is higher (see Figure 4 a. and b.). Perhaps the assumptions for Theorem 4 are violated for these adversarially trained models. \n\n1. https://arxiv.org/abs/1807.06732\n2. https://arxiv.org/abs/1807.01697\n3. https://arxiv.org/abs/1608.08967\n4. https://openreview.net/forum?id=S1xoy3CcYX&noteId=BklKxJBF57.\n5. https://arxiv.org/abs/1704.08847\n6. https://arxiv.org/abs/1608.07690\n7. https://arxiv.org/abs/1611.01232\n8. https://arxiv.org/abs/1806.05393\n9. https://arxiv.org/abs/1712.09665\n10. https://arxiv.org/abs/1804.11285\n11. https://arxiv.org/pdf/1809.02104.pdf", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, {"title": "A solid contribution to the study of adversarial examples.", "review": "The authors provide a compelling theoretical explanation for a large class of adversarial examples.  While this explanation (rooted in the norm of gradients of neural networks being the culprit for the existence of adversarial examples) is not new, they unify several old perspectives, and convincingly argue for genuinely new scaling relationships (i.e. \\sqrt(d) versus linear in d scaling of sensitivity to adversarial perturbations versus input size). They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures, and provide thorough empirical evidence of their work.\n\nI can honestly find very little to complain about in this work--the prose is clear, and the proofs are correct as far as I can tell (though I found Figure 4 in the appendix (left panel) to not be hugely compelling.  More data here would be great!)\n\nAs much of the analysis hinges on the particularities of the weight distribution at initialization, could the authors comment on possible defenses to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)?", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"title": "interesting work but with limited applicability and significance demonstrated", "review": "This paper analyzes the relationship between \"adversarial vulnerability\" with input dimensionality of neural network. The paper proves that, under certain assumptions, as the input dimensionality increases, neural networks exhibit increasingly large gradients thus are more adversarially vulnerable. Experiments were done on neural networks trained by penalizing input gradients and FGSM-adversarial training. Similar trends on vulnerability vs dimensionality are found.\n\nThe paper is clearly written and easy to follow. I appreciate that the authors also clearly stated the limitation of the theoretical analysis.\n\nThe theoretical analyses on vulnerability and dimensionality is novel and provide some insights. But it is unlikely such analysis is significant There are a few reasons:\n- This analysis only seems to work for \"well-behaved\" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is not clear that how this will apply. (and I appreciate that the authors also acknowledge this in the paper.) It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. After all, the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem.\n- One very important special case of the point above: the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones. Such models may have small gradients inside the box constraint, but can have large gradients between different classes.\n\n\nOn the empirical results, the authors made a few interesting observations, for example the close correspondence between \"Adv Train\" and \"Grad Regu\" models. \nMy concern is that the experiments were done on a narrow range of models, which only have \"weak\" adversarial training / defenses.\nAdversarial robustness is hard to achieve. What matters the most is \"why the strongest model is still not robust?\" not \"why some weak models are not robust?\" \nIt is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models [MMS+17] which is the SOTA on MNIST/CIFAR10 L_\\infty robustness benchmark.\nWithout comprehensive analyses on SOTA robust models, it is hard to justify the validity of the theoretical analysis in this paper, and the conclusions made by the paper.\nFor example, re: the last sentence in the conclusion: \"They hence suggest to tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques.\" The reasoning is not obvious to me given the current evidence shown in the paper.\n\n[MMS+17] Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}], "Metareview": "This paper suggests that adversarial vulnerability scales with the dimension of the input of neural networks, and support this hypothesis theoretically and experimentally. \n\nThe work is well-written, and all of the reviewers appreciated the easy-to-read and clear nature of the theoretical results, including the assumptions and limitations. (The AC did not consider the criticisms raised by Reviewer 3 justified. The norm-bound perturbations considered here are a sufficiently interesting unsolved problem in the community and a clear prerequisite to solving the broader network robustness problem.) \n\nHowever, many of the reviewers also agreed that the theoretical assumptions - and, in particular, the random initialization of the weights - greatly oversimplify the problem. Reviewers point out that the lack of data dependence and only considering the norm of the gradient considerably limit the significance of the corresponding theoretical results, and also does not properly address the issue of gradient masking. ", "URL": "https://openreview.net/forum?id=H1MzKs05F7", "ID": "H1MzKs05F7", "Venue": "ICLR-2019"}
{"Title": "Policy Optimization in Zero-Sum Markov Games: Fictitious Self-Play Provably Attains Nash Equilibria", "Authors": ["Boyi Liu", "Zhuoran Yang", "Zhaoran Wang"], "Abstract": "Fictitious Self-Play (FSP) has achieved significant empirical success in solving extensive-form games. \nHowever, from a theoretical perspective, it remains unknown whether FSP is guaranteed to converge to Nash equilibria in Markov games.\nAs an initial attempt, we propose an FSP algorithm for two-player zero-sum Markov games, dubbed as smooth FSP, where both agents adopt an entropy-regularized policy optimization method against each other. \nSmooth FSP builds upon a connection between smooth fictitious play and the policy optimization framework. Specifically, in each iteration, each player infers the policy of the opponent implicitly via policy evaluation and improves its current policy by taking the smoothed best-response via a proximal policy optimization (PPO) step. \nMoreover, to tame the non-stationarity caused by the opponent, we propose to incorporate entropy regularization in PPO for algorithmic stability. \nWhen both players adopt smooth FSP simultaneously, i.e., with self-play, we prove that the sequence of joint policies converges to a neighborhood of a Nash equilibrium at a sublinear $\\tilde{O}(1/T)$ rate, where $T$ is the number of iterations. To our best knowledge, we establish the first finite-time convergence guarantee for FSP-type algorithms in zero-sum Markov games.", "Review": [{"title": "Important topic. Missing key related work. The weight carried by assumptions can be discussed more.", "review": "This paper is about the design and analysis of policy optimization algorithms that provably converge to a Nash equilibrium at a sublinear rate for a class of zero sum Markov games, which are one of the simplest settings of multi-agent RL --- in particular, for zero sum Markov games satisfying a Lipschitz regularity condition. Each players adopts an entropy-regularized policy optimization method (which the authors call as smooth Fictitious Self Play).\n\nThis is an important topic and the question studied is an important step to take given that we don't know whether Fictitious Self Play is guaranteed to converge in Markov games. However, I am quite surprised by very important related work missing in the paper. For instance, the NeurIPS 2019 paper on \"Policy Optimization Provably Converges to Nash Equilibrium in Linear Quadratic Games\" is not cited even though it is quite close to the topic of this paper: it also studies a policy optimization algorithm, linear quadratic games are also zero-sum Markov games, the objective studied is also non-convex non-concave. Of course LQ games are special class of zero-sum Markov games, but this paper makes some assumptions like Lipschitz regularity as well. Therefore claims like this paper is the first to prove convergence guarantees of policy optimization algorithms for zero sum Markov games are not quite true. The somewhat less related, but still quite close NeurIPS 2019 paper \"Model-based multi-agent RL in zero-sum Markov games with near optimal sample-complexity\" is not cited as well. These papers are not obscure: a simple search for the submitted paper's title brings up these papers.\n\nAlso, the Lipschitz regularity assumption being made is important enough that it is good to add it in the abstract, as the abstract feels misleading otherwise. And the importance/restrictiveness of this assumption is ideally discussed in more detail in the introduction. \n\nOverall I think this would make a good paper after fixing the above, but not right now. ", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, {"title": "A nice result on a hard problem", "review": "This paper studies the problem of learning to play a Nash equilibrium in two-player, zero-sum Markov games.  This is a longstanding problem, with many algorithms proposed but relatively few theoretical convergence guarantees, and most of those either for quite restricted settings or with strong assumptions.  This is in stark contrast to the stateless setting of Normal Form Games, where we have many strong theoretical convergence guarantees.  The main algorithm is a version of the classic fictitious play algorithm.  Like prior adaptations of fictitious play to Markov Games, it operates on the Q-values, but a key novelty (at least in the stateful setting; similar ideas were recently applied in a special case of normal form games by Swenson and Poor 2019) is the use of a particular form of regularization in the best response process.  The main result is that as long as the game satisfies Lipschitz and Concentratability properties for each player when the other plays optimally and the policy updates are sufficiently accurate then play converges to a Nash equilibrium.\n\nI like this paper quite a bit.  It tackles a hard problem  and makes solid progress.  I think the algorithm and analysis are both nice contributions and definitely intend to study the latter further as I think aspects of it may be useful in other settings.  Overall the presentation, while dense, is clear.  However, I believe there are a few issues that could use additional discussion:\n\n 1) Why does the uniqueness, or lack thereof, of the Nash equilibrium not matter to the results?  Quite a bit of prior work has had caveats when they are not unique.  The results seem to hold if the assumptions are true for at least one equilibrium, presumably because of the minimax properties in a zero-sum setting, but I\u2019m not quite clear how this interacts with the assumptions.  For example, if one but not all the equilibra cause the game to satisfy Assumption 4.2 and 4.3 what causes the guarantees to still hold even if initially play gravitates toward some equilibrium where they do not?\n\n2) I\u2019m not quite clear how to interpret the convergence guarantee in Theorem 4.5.  The text after the theorem talks about the policy sequence converging to a neighborhood, while the theorem itself is about the averages across the sequence of policies.  It would help to have some more detailed discussion of exactly what sort of convergence behavior we should expect.\n\n3) I\u2019m intrigued by the observation at the end that this algorithm is Hannan consistent under stronger assumptions.  There has been some work recent work exploring connections between regret minimization and RL and it would be worth discussing a bit how this observation relates to that literature, e.g.: \n\n@inproceedings{hennes2020neural,\n  title={Neural Replicator Dynamics: Multiagent Learning via Hedging Policy Gradients},\n  author={Hennes, Daniel and Morrill, Dustin and Omidshafiei, Shayegan and Munos, R{\\'e}mi and Perolat, Julien and Lanctot, Marc and Gruslys, Audrunas and Lespiau, Jean-Baptiste and Parmas, Paavo and Du{\\`e}{\\~n}ez-Guzm{\\'a}n, Edgar and others},\n  booktitle={Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},\n  pages={492--501},\n  year={2020}\n}\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"title": "Review: Policy Optimization in Zero-Sum Markov Games: Fictitious Self-Play Provably Attains Nash Equilibria", "review": "The authors consider self-play in zero-sum discounted two-player Markov games with compact state space and finite actions. They present a smooth fictitious self-play algorithm where each player adopts an entropy-regularized policy optimization method with the average of the past generated Q-values. Under appropriate assumptions, among which a Lipschitz regularity of the Markov game, the authors prove that this algorithm approximates the Nash equilibrium at a rate O(1/T) where T is the number of iteration.\n\n-Contributions\n\n-algorithmic: Smooth FSP algorithm a smooth version of fictitious self-play.\n-theoretical: Convergence rate of order O(1/T) of Smooth FSP under appropriate conditions.\n\n-Score justification/Main comments\n\nThe paper is well written. The proofs seem correct but I did not check everything in detail (see specific comments). My main concern is that the different assumptions made are a bit ad hoc (sometimes the assumption relates directly on the sequence of policies generated by the algorithm). And thus it is hard to assess if the provided bound is relevant or a trivial consequence of the assumptions (see specific comments below). As a sanity check and for a simpler proof it could be interesting to first present and analyze Smooth FSP without the estimation and approximation part first. \nIn fictitious play, each player plays the best response against the average of the past policies played by the adversary. Here it is not really the case since the policy used by one player is a  (smooth) best-response against a weighted average of the past Q-value, which depends also on the policy played by that player. Thus the link with Fictitious play is not completely clear. \nI\u2019m also curious about the reduction of the presented algorithm to matrix game. Do we recover a known algorithm, and what can we say about the convergence rate of the algorithm? \n\n \n\n-Detailed comments \nP2: \u201cremains less less understood\u201d and what do you mean by \u201cclassical optimization\u201d?\n\nP4, Section 3.1: the mixed policy as you defined it is not a policy (you cannot express as a certain function of s) thus talking about its Q-value does not make sense.\n\nP5: Is the normalization parameter \\kappa_{t+1,(i)} a learning rate of the normalization constant such that the probability sum to one? In the second case, it should be additive.\n\nP6: I do not understand the last sentence before Section 3.3 what do you mean by obtained from (3.3) and \u201cwhich operates in the functional space given the marginalized\u201d?\n\nP5, Section 3.3: \\Theta is not defined, how do you parameterized \\cE_{\\theta} exactly? What do you mean by \u201cthe estimator of the marginalized[...]\u201d, how do you construct it?  \n\nP13, Appendix A: maybe you should say that you consider the Lagrangian on the constrained optimization problem and if it the case also add the constraint on the fact that the \\pi(a|s)\\geq 0.\n\nP6: In (4.3), you mean when \\nu_t is close enough to \\nu^*?\n\nP6, Assumption 4.2: could you provide a non-trivial example where this assumption is correct? Furthermore, the assumption is made on the algorithm that you propose rather than on the model is very suspicious. In particular, since the sequence \\pi_{\\theta^t} is a random sequence (because based on estimated quantities) in which sense the inequality holds? Almost surely?\n\nP7, (4.7): h is an integrable function with respect to which measure? And similarly, the L1-norm is defined with which measure?\n\nP7, Assumption 4.3: Could you provide a non-trivial example where this assumption is correct? And this assumption is not weaker than the one proposed by Radanovi et al because the quantities E_{\\nu^*}[ KL(\u2026)] and max_{s} || ...||_1 are not comparable. \n\nP7, Assumption 4.4: Again since you are manipulating random quantities you should precise about what you mean by this inequality. Furthermore, there is in fact no assumption here but just introducing the notations \\epsilon_t and \\epsilon_t\u2019 (if we allow them to be in \\bar{R}). The assumption would rather be that \\sigma = O(1). And I\u2019m not totally convinced it is a reasonable assumption. For example in (4.1) if \\nu^t is singular with respect to \\nu^* then the MSE computed with \\nu_t will provide no information for the state where \\nu^* is supported.\n\nP8, Theorem 4.5: In fact M_i depends on \\lambda_i trough V_{(i)}^{max}\u2026, thus it is not clear at all if you can set \\lambda_i \\geq 2M_i. In fact, you are adding an additional constraint on the different parameters. Could you make this explicit in the statement of the theorem? \n\nP16: In (C.10) in seems that you used t+1-(\\lambda_i+M_i)/(max_{i} \\lambda_i+M_i) \\leq t which is wrong.\n\nP16: In (C.11) the left-hand side should be divided by T.\n\nP23: In (G.2) it should be \\log(\\bar{\\pi}^i_{t+1}) instead of \\log(\\pi^i_{t+1})?\n\nP23, end of the proof of Lemma C.4: it is \\tilde{Q}.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"title": "Well written and interesting paper", "review": "This paper studies the two-player zero-sum Markov game using fictitious self-play (FSP) strategies. The authors proposed a novel entropy regularized policy optimization method for both agents. They proved the sequence of joint policies converges to a neighborhood of a Nash equilibrium at a sublinear rate. The paper is well written though the notations are a little bit complicated for readers to understand. The results seem to be rigorous. \n\nOne drawback is that the proposed algorithm is not evaluated using any empirical studies. Since the algorithm is new to the literature, it would be expected to see how it performs compared with other baseline methods in experiments. \n\nHave you considered the stochastic variance reduced policy gradient methods? There has been an active line of work (see [1-5] for some examples) that shows the variance reduction techniques can improve the convergence rate of policy optimization methods in the single-agent setting. It would be interesting to know whether the convergence of the smooth FSP can be also improved using the same techniques.\n\n[1] Papini M, Binaghi D, Canonaco G, Pirotta M, Restelli M. Stochastic Variance-Reduced Policy Gradient. InInternational Conference on Machine Learning 2018.\n[2] Xu P, Gao F, Gu Q. An improved convergence analysis of stochastic variance-reduced policy gradient. In Uncertainty in Artificial Intelligence 2019.\n[3] Shen Z, Ribeiro A, Hassani H, Qian H, Mi C. Hessian aided policy gradient. In International Conference on Machine Learning 2019.\n[4] Xu, P., Gao, F. and Gu, Q. Sample Efficient Policy Gradient Methods with Recursive Variance Reduction. In International Conference on Learning Representations 2020.\n[5] Huang F, Gao S, Pei J, Huang H. Momentum-Based Policy Gradient Methods. In International Conference on Machine Learning 2020.\n\nEquation (2.3) is not entropy regularized. Instead, the state-reward function is entropy regularized.\n\nHow is the mean squared error in (3.7) solved? In the proposed algorithm, it is assumed that this can be exactly solved. However, a practical approximation of this solution will cause additional estimation error. As is required in Equation (4.13), it seems that the authors assume the estimation error to be roughly in the order of 1/t^2. I am not sure whether this strong convergence can be established using sampled data for (3.7).\n\nThe convergence result in Theorem 4.5 is upper bounded by a very large term \\lambda_i \\log|A^i|, where \\lambda_i is larger than the Lipschitz constant, and |A^i| is the size of the action space. If I understand it correctly, both quantities are nonvanishing and thus the result in Theorem 4.5 is not convergent. I did not see any discussion in the paper to address this issue or discuss how the neighborhood can be shrunk to a smaller region. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}], "Metareview": "The paper shows that a form of Fictitious Self-Play converges to the Nash equilibria in Markov games. Understanding the theoretical properties of Fictitious Self-Play is important, however the paper in its current form is not ready for publication. The paper needs a more thorough discussion on related works, the assumptions made, and as pointed out by Reviewer3, the convergence argument needs to be expanded and explained in more detail. Further, I encourage authors to add experiments and compare their algorithm with other methods. ", "URL": "https://openreview.net/forum?id=c3MWGN_cTf", "ID": "c3MWGN_cTf", "Venue": "ICLR-2021"}
{"Title": "Divide and Contrast: Source-free Domain Adaptation via Adaptive Contrastive Learning", "Authors": ["Ziyi Zhang", "Weikai Chen", "Hui Cheng", "Zhen Li", "Siyuan Li", "Liang Lin", "Guanbin Li"], "Abstract": "We investigate a practical domain adaptation task, called source-free domain adaptation (SFUDA), where the source pretrained model is adapted to the target domain without access to the source data. Existing techniques mainly leverage self-supervised pseudo-labeling to achieve class-wise global alignment [1] or rely on local structure extraction that encourages the feature consistency among neighborhoods [2]. While impressive progress has been made, both lines of methods have their own drawbacks \u2013 the \u201cglobal\u201d approach is sensitive to noisy labels while the \u201clocal\u201d counterpart suffers from the source bias. In this paper, we present Divide and Contrast (DaC), a new paradigm for SFUDA that strives to connect the good ends of both worlds while bypassing their limitations. Based on the prediction confidence of the source model, DaC divides the target data into source-like and target-specific samples, where either group of samples is treated with tailored goals under an adaptive contrastive learning framework. Specifically, the source-like samples are utilized for learning global class clustering thanks to their relatively clean labels. The more noisy target-specific data are harnessed at the instance level for learning the intrinsic local structures. We further align the source-like domain with the target-specific samples using a memory bank-based Maximum Mean Discrepancy (MMD) loss to reduce the distribution mismatch. Extensive experiments on VisDA, Office-Home, and the more challenging DomainNet have verified the superior performance of DaC over current state-of-the-art approaches. The code is available at https://github.com/ZyeZhang/DaC.git.", "Review": [{"recommendation": "Accept", "confidence": "Certain", "award": "No"}, {"rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.", "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.", "questions": "1. When do the method update the source-like set and class centroids, after one batch or one epoch\uff1f", "ethics_flag": "No", "ethics_review_area": [], "soundness": "3 good", "presentation": "3 good", "contribution": "3 good", "code_of_conduct": "Yes", "review": "The authors proposed a method for source-free unsupervised domain adaptation (SFUDA) task. The key idea is to combine the advantages of global alignment [1] and feature consistency [2]. The authors divided the target data into source-like and target-specific samples and \ntreat them by different learning methods. The authors demonstrate extensive experiments on three datasets and verify the performance of the proposed method Strengths\n1.The paper is well-organized and easy to follow.\n2. The \"divide and contrast\" strategy is simple but effective; it can exploit both the global and local structures of target data.\n3. The proposed Exponential-MMD loss is novel, it also makes sense to align the source-like and target-specific samples to reduce distribution mismatch.\n4. The experiment and ablations are sufficient to support the conclusion.\n\nWeaknesses\n1. The section of preliminaries and analysis is a bit unclear to introduce the task.\n2. A more recent work that outperforms proposed approach ([a]) is not compared against.\n[a] Liang, Jian, et al. \"Source data-absent unsupervised domain adaptation through hypothesis transfer and labeling transfer.\"\u00a0IEEE Transactions on Pattern Analysis and Machine Intelligence\u00a0(2021).\n3. Eqn. 4 and Eqn. 5 share the same parameter \\tau but they are different from each other. The threshold \\tau in Eqn. 4 is set to 0.95 according to supplementary material following [47], however, I could not find the reference [47]. Besides, the choice of the threshold \\tau is not included in the ablation study, it would be necessary to see the ablation of choosing the threshold \\tau.\n4. As described in Discussion(L200-L207), unlike previous method [3], Eqn. 5 jointly achieves class-wise adaptation and instance-wise adaptation. It would be interesting if the authors could compare the proposed loss with separate class-wise adaptation and instance-wise adaptation losses.\n5. When do the method update the source-like set and class centroids, after one batch or one epoch. Yes "}, {"rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.", "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.", "questions": "See weaknesses above.", "ethics_flag": "No", "ethics_review_area": [], "soundness": "3 good", "presentation": "3 good", "contribution": "3 good", "code_of_conduct": "Yes", "review": "This paper presents a source-free domain adaptation method. Previous methods either use self-supervised pseudo labeling to conduct class-wise global alignment or leverage local structure to enforce feature consistency. This work combines the idea of both. The proposed method divides target samples into source-like and target-specific ones. Source-like samples are used for global class clustering and target-specific samples are used for learning local structures. The two are further aligned using maximum mean discrepancy loss. Strengths are as follows.\n\nThe idea is interesting. The target samples are divided based on confidence output of source classifier. Different groups are treated differently, either globally in class-level or locally in instance-level. Two different groups are aligned to encourage consistency that is also interesting. The presentation is generally good. Ablation study is conducted for each part.\n\nWeaknesses are as follows.\n\nThe major weakness is the performance compared to prior work. It shows that the results of the proposed method are just marginally good compared to [2] and [3] in 2 out of 3 datasets (table 1 and table 3). \n\nThe performance is good in table 2. Can the authors explain more about the baseline results? For example, how the official code was used and how the authors ensure the hyper parameters are reasonably well tuned? \n No "}, {"rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.", "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.", "questions": "1. Line 262: change \u201ctrains\u201d to \u201ctrain\u201d\n2. Any reason for missing the experiments on Digit datasets and the Office31 dataset?\n", "ethics_flag": "No", "ethics_review_area": [], "soundness": "3 good", "presentation": "4 excellent", "contribution": "2 fair", "code_of_conduct": "Yes", "review": "This paper proposes a new source-free unsupervised domain adaptation method named DaC. The key idea is to leverage the advantages of existing \u201cglobal\u201d methods and \u201clocal\u201d counterparts. Specifically, DaC uses the source model to split the target data into source-like and target-specific samples. After that, an adaptive contrastive learning strategy is used to achieve class-wise adaptation (global) and local consistency (local). Finally, MMD is used to minimize the distribution mismatch between source-like samples and target-specific samples. The proposed method achieves the best performance on widely used benchmarks.\n ---\n\nOriginality: the proposed method is a combination of well-known techniques, but achieves good performance. Integrating contrastive learning into source-free domain adaptation is novel and brings new insights into this community.\n\n---\n\nQuality: \n\nStrengths: this work is technically sound with theoretical proof and empirical evaluation. The effectiveness of the proposed regularizations are evaluated on various downstream tasks. The work is complete.\n\nWeakness: (1) the major concern is the performance. As shown in Table 1 and Table 3, compared to existing SOTA methods (NRC and CPGA), the performance improvement is quite limited. Although it outperforms the baseline SHOT by a large margin, it is still hard to convince readers. (2) The proposed method is not evaluated on Digit datasets (MNIST, SVHN, and USPS) and the Office31 dataset which are two important benchmarks. Any reason behind this?\n\n---\n\nClarity: this paper is well written and well organized. It is easy to follow. Detailed implementation details are also provided in the supplementary.\n\n---\n\nSignificance: compared to the baseline SHOT, the result is important. Specifically, the proposed method brings a larger improvement than SHOT. However, it is not clear to me whether other methods can borrow the same idea and get performance improvement.\n\n---\n Not applicable.\n "}, {"rating": "8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.", "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.", "questions": "1. Line 51: Provide a brief explanation for the \"memory bank\".\n2. Line 260: What is SHOT? Cite a reference for that.\n", "ethics_flag": "No", "ethics_review_area": [], "soundness": "3 good", "presentation": "3 good", "contribution": "3 good", "code_of_conduct": "Yes", "review": "This paper is about unsupervised domain adaptation when the source data is unavailable at the time of adaptation. Authors term this problem as source-free unsupervised domain adaptation (SFUDA). Authors make the insightful  observation that existing SFUDA techniques use all pseudo labels on target data including noisy labels or enforce local feature consistency at the expense of being source-biased. Motivated by these observations, authors propose a new SFUDA approach that they call Divide and Contrast (DaC) which divides the target data into two disjoint groups: source-like samples and target-specific samples. An adaptive learning framework is presented that treats each of these target sample groups differently during the training process. Authors present both theoretical results and numerical results that illustrate the superiority of DaC over state of the art methods for SFUDA. Strengths\nAuthors illustrate well (e.g., Fig. 1) the tradeoffs involved in the global and local approaches to SFUDA.\n\nThe proposed DaC approach for SFUDA is a novel approach to address the limitations of prior methods to SFUDA.\n\nAuthors present theoretical derivations to justify the DaC approach.\n\nNumerical results shown on multiple datasets are pretty convincing of the superiority of DaC approach.\n\nWeaknesses\n\"Memory bank\" is invoked early on without sufficient explanation. It is explained in Section 4.2.2, but may make the paper more readable if this introduction could be provide earlier in the manuscript.\n\nNo error bars are provided for the numerical results. Authors do not explicitly discuss the limitations of the proposed method, but mention semi-supervised SFUDA  and source-free open-set DA as possible research extension topics. If space permits, it may be interesting to provide a few more sentences about each of these topics and how DaC might fare in those cases. "}], "Decision": "Accept", "Metareview": "This paper proposes a relatively complicated method for source-free unsupervised domain adaptation, which integrates several techniques into a divide and contrast framework. The idea of dividing the target data into source-like subset and target-specific subset and employing global alignment and feature consistency for each subset is novel when the source data is inaccessible. The contrastive learning and memory-based MMD are novel in the context of source-free domain adaptation and introduce theoretical benefits in terms of the expansion theory and domain alignment theory, respectively. Reviewers were on the positive side while holding some concerns on the marginal improvement over the SoTA methods, which were addressed in the author rebuttal. AC generally agreed that the paper has introduced a novel and solid contribution to the field, with a nice connection between algorithmic methods and theoretical insights, and  recommended the paper for acceptance. Authors are suggested to incorporate all rebuttal material in the revision and if possible, to work out a recipe for easing the adoption of their relatively complicated framework that comes with many modules and loss terms.", "URL": "https://openreview.net/forum?id=NjImFaBEHl", "ID": "NjImFaBEHl", "Venue": "NeurIPS-2022"}
{"Title": "Partial Rejection Control for Robust Variational Inference in Sequential Latent Variable Models", "Authors": ["Rahul Sharma", "Soumya Banerjee", "Dootika Vats", "Piyush Rai"], "Abstract": "Effective variational inference crucially depends on a flexible variational family of distributions. Recent work has explored sequential Monte-Carlo (SMC) methods to construct variational distributions, which can, in principle, approximate the target posterior arbitrarily well, which is especially appealing for models with inherent sequential structure. However, SMC, which represents the posterior using a weighted set of particles, often suffers from particle weight degeneracy, leading to a large variance of the resulting estimators. To address this issue, we present a novel approach that leverages the idea of \\emph{partial} rejection control (PRC) for developing a robust variational inference (VI) framework. In addition to developing a superior VI bound, we propose a novel marginal likelihood estimator constructed via a dice-enterprise: a generalization of the Bernoulli factory to construct unbiased estimators for SMC-PRC. The resulting variational lower bound can be optimized efficiently with respect to the variational parameters and generalizes several existing approaches in the VI literature into a single framework. We show theoretical properties of the lower bound and report experiments on various sequential models, such as the Gaussian state-space model and variational RNN, on which our approach outperforms existing methods.", "Review": [{"title": "SMC for VI", "review": "This paper describes an SMC algorithm to sample the posterior distribution of latent states $p_\\theta(z_{1:T}|x_{1:T})$ in a latent variable models $p_\\theta(x_{1:T}, z_{1:T})$. The authors consider a completely general setting (the authors assume Eq.(1) but clearly there is nothing to assume here, this the standard Bayes rule). It is well known that the vanilla SMC sampler is a good candidate for ELBO because it provides an unbiased estimator of the likelihood. But the authors prefer here to use a more sophisticated version of the  SMC algorithm, which features a partial rejection algorithm, which amounts to eliminate proposed particles which are \"large enough\" likelihood.\nIt is difficult for an expert in SMC algorithms to understand the algorithm as it is described (one must even guess the meaning of the notations). Equation 3) is rather misleading because it is not understood that one continues the acceptance/rejection sampling step until the sample is accepted [Peters' paper, a little less ambitious in its generality, is much more readable]. Also, the form of the rejection probability does not help to understand the action beyond the scene. The level of generality here is a killer: what the hyperparameter $M(i,t-1)$ means, what is the meaning of this form of rejection probability (we see something like a Barker ratio), this is very puzzling\nThe rejection probability modifies the mutation kernel and should be taken into account when computing the importance weights (Eqs. (4) and (5)). This implies to estimate a quantity $\\alpha_t(z_{1:t}^i)$ which is not tractable. The authors suggest (similar to Peters (2012)) to use a Monte Carlo estimator of these quantities. \nTo sample the ancestors variables from Eq. (7) with the $\\alpha_t(z_{1:t}^i)$ defined in Eq. (6), the authors use the \"Dice-Enterprise\" algorithm. It does not seem necessary to appeal to such  beautiful algorithm to understand the validity of the algorithm described page 3. Here again, everything is done to frighten the reader and not much is done to explain what is being done... \n\nThe results presented are encouraging and shows that the proposed approach outperforms IWAE and FIVO for a given calculation time. This result is clearly interesting and shows that partial rejection helps despite the additional difficulties linked with the intractability which requires an additional layer of complexity. \n\nI like the paper even if I have found it extremely unfriendly to read ! \n ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, {"title": "Official Blind Review #3", "review": "### Review update following author discussion\n\nI've read the author responses as well as some of the discussion with the other reviewers. Overall, this is valuable work and I've considered raising my score, but I think a weak accept is appropriate, all things considered.  I've raised the confidence score for my review, as I understand the technical details better now. I think a key strength compared to prior work is the empirical validation of the approach on a variational RNN.  However, the significance of the paper in terms of the novelty of the ideas, both conceptual and technical are overstated in my opinion, hence the weak accept. \n\nOne other point of feedback for the final version in case of an accept:\n\nI also share R3's concern about the paper's positioning in relation to the extremely general dice enterprise framework (or, even the bernoulli factory, for that matter) as somewhat misleading for the particular use case in Section 2.2.  The exact same multinomial sampling scheme is in fact more succinctly presented (proposed?) in BRPF [Schmon et. al, 2019] as the \"Bernoulli Race\" (which the authors have cited).  I would think that the current scheme is a special case of the \"Bernoulli race\" that uses a particular form of the acceptance probability parameterization similar to prior work (e.g. VRS [Grover et. al. 2018]). \n\nSee Section 3, specifically e.g. Eq (10) and Proposition 2 from BRPF [Schmon et. al., 2019], which can be compared to Eq (3) and Proposition 1 in the current submission, respectively. \n\nMinor nit: In step 3 of the algorithm (right below Eq (8)), you use the notation $z_t$ for sampling a new variable from $q_\\phi$. However, this $z_t$ has nothing to do with the latent variables that are used in computing the constants $c^i_t$. The way it's written makes it appear as if there's a circular depdendence of the $c_t$ on $z_t$ and then the $z_t$ is again resampled, which changes the $c_t$. For this reason, it maybe better to use a completely unrelated variable for the sample from $q_\\phi$ in step 3. \n\n\n### Key Strengths\n\nThe paper puts together several ideas from prior works (partial rejection control/SMC, variational inference, dice-enterprise) and also evaluates these ideas for latent variable sequential state space model benchmarks. The experimental results compare favorably to prior works like FIVO and IWAE and demonstrate that using partial rejection control is beneficial in a variety of benchmarks. \n\n### Key Weaknesses\n\nIt seems like a key contribution in terms of novelty/theory is in fixing the bias in prior works using Partial Rejection Control in SMC (e.g. Peters et. al. 2012). More information to support this claim in terms of why the prior estimate is biased would be helpful in assessing the strengths of the paper's contributions (Peters et. al don't seem to explicitly focus on the exact bias for PRC).  Having said that, the experiments seem to show that unbiased gradients are worse than a biased version (Figure 2, left bottom), so it seems like focusing on the exact bias is not that important.  \n\n\n### Additional Comments\n\n* The proof of unbiasedness (Prop 2) says \"it is easy to show that Eq (15) is an unbiased estimator\" and refers to prior work by [Naesseth et. al] for details. More clarity here would be helpful (especially around the term $Q_{VSMC-PRC}$) for assessing this claim, given that this is one of the main contributions claimed in the paper (besides also commenting on where the bias in prior work is coming from). \n\n* The $Z()$ function first appears in Equation (9), without a prior reference/definition. You might want to introduce this around Eq (4), where the integral appears. There's also a reference to what will later be $Z$ as $p^i_t$ in Eq (8).\n\n* Using partial forms of rejection to proposal distribution samples specifically for variational inference (rather than SMC more generally) has also been considered in prior work [R. Gummadi, \"Resampled Belief Networks for Variational Inference\", Advances in Approximate Bayesian Inference Workshop, 2014]. \n\n\n\n\n\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"title": "Official Review 2", "review": "Summary:\nThe submission suggests a new variational bound for sequential latent variable models. Unlike previous work that optimize this bound using \u2018standard\u2019 particle filters with unbiased resampling, the new bound is constructed based on a partial rejection control step and uses a dice enterprise for sampling the ancestor variables. \n\nPositives:\nThe combination of partial rejection control and dice enterprise for variational inference is new and interesting. Particle filters with partial rejection control have been used before for constructing (biased) bounds based on the marginal likelihood. However, using a dice-enterprise step allows for a new unbiased bound which makes it possible to consider a lower bound on the log-likelihood via variational ideas that can be optimized with standard techniques. Empirical experiments suggest that the method outperforms previous work.\n\nNegatives:\nDoes the complexity of the new bound not scale linearly with K (while K=1 for FIVO)? This seems to be not accounted for in the experiments. Choosing larger N=16 also has a better performance in the FIVO paper. \n\nRecommendation:\nI vote for acceptance of the paper. However, I think that the experimental section should be improved.\n\nComments:\nVariational bounds can also be constructed by targeting a smoothing distribution (Lawson et al, 2019) and particle filters with complexity N^2 based on a marginal Fisher identity have been suggested (POYIADJIS et al, 2011) for parameter estimation that avoid estimator variances scaling quadratically in time. I was wondering if there is a connection between such filters and the method suggested here, particularly for K=N?\nCan you explain the connection between the variance of the estimator for the normalizing constant obtained from particle filters and the tightness of the variational bound in more details?\nAre the signal-to-noise gradient issues for large N or K?\nHow do the methods in the experiments compare for a larger number of particles?\nIs there some useful practical advice on choosing the ratio N/K and gamma?\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"title": "A fine paper on Sequential Monte Carlo with partial rejection control, but the question might have been tackled in the literature already ", "review": "The paper considers SMC to construct variational approximations. SMC methods\ncan be improved with partial rejection control (PRC). Then it is not obvious\nthat one can obtain unbiased estimators of the normalizing constant, as in\nplain SMC. The authors consider a way of obtaining unbiased estimators in that\nsetting.  Experiments include a linear Gaussian state-space model and recurrent\nneural networks on polyphonic music datasets.\n\n---\n\nThe problem is interesting. The notation is a bit cumbersome at times, but it is \nthe case for most papers on SMC. The writing is mostly clear. The experiments\ninclude a mix of toy and more realistic examples. \n\nThe fact that SMC with PRC can still produce unbiased estimators of the\nmarginal likelihood was described by Kudlicka, Murray, Schoen, Lindsten,\n\"Particle filter with rejection control and unbiased estimator of the marginal\nlikelihood\". The authors do cite that paper, but I did not understand exactly\nwhy that paper does not completely solve the problem that the authors consider.\nAt first glance it seems that the Kudlicka et al paper would apply \"out of the\nbox\" when replacing the prior by an arbitrary proposal in SMC, i.e. using a\nproposal \"$q_t$\" instead of \"$f_t$\" and the ratio \"$g_t f_t/q_t$\" for the weights.\nMy understanding is that Kudlicka et al focus on the bootstrap particle filter\nbecause PRC is a remarkably generic approach to improve it, applicable even\nwhen sampling from the model prior is the only option; not because the same\nreasoning would not apply for generic proposals. \n\nThus it was not clear to me that there's a need for another paper showing that\nSMC with PRC can still provide unbiased estimators of the marginal likelihood.\nIf the authors made a convincing case that the extension to general proposals\nwithin SMC with PRC requires significant work, their contribution would be more\nconvincing.\n\nFurthermore, the manuscript makes references to Bernoulli factories and dice\nenterprise.  In fact, the manuscript addresses the problem of categorical\nsampling with unbiased estimators of the underlying probabilities.  The problem\nwas addressed in \"Bernoulli Race Particle Filters\" by Schmon, Doucet,\nDeligiannidis.  That paper is cited by the authors, but the authors do not make\nit clear that their algorithm (in Section 2.2) is exactly the same as\n\"Algorithm 2\" of that paper, their Proposition 1 is exactly \"equation (12)\" of\nthat paper, etc.  Furthermore, I don't think their algorithm is indeed a \"dice\nenterprise\" as in the terminology of Morina et al; I believe the references to\nthe Bernoulli factory literature would be sending most readers in the wrong\ndirection.\n\nBased on these flaws I do not think that the manuscript is suitable for\npublication. Perhaps a clearer explanation of the specific shortcomings of\nKudlicka et al's work would make the paper more convincing.\n\n---\nSmall comments:\n\n- page 2: \"We further assume that the joint density [...]\"\nin fact that decomposition always holds, it is not an assumption.\nIt is just p(a,b) = p(a)p(b|a). \n\n- page 2: A SMC sampler -> An SMC sampler\n\n- page 2 and later: there is some inconsistency between the notation M(i,t-1) and M(t-1,i).\n\n- The latex command \\eqref seems to have been used instead of \\ref, in various places.\n\n- page 6 \"utlizing the best of both worlds\"\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}], "Decision": "Reject", "Metareview": "This paper explores the use of partial rejection control (PRC) for improved SMC-based variational bounds. While an unbiased SMC variant with PRC has been previously introduced by Kudlicka et al. (2020), this work introduces innovations that can help apply such ideas to variational inference. These bounds result in improvements in empirical performance. \n\nThis paper was heavily discussed, with significant engagement by both the authors and the reviewers. Most reviewers recommended acceptance of this paper, with one reviewer (R4) recommending against acceptance. R4's central concerns regard the novelty of the proposed approach and its positioning relative to the existing SMC literature. The authors argued vigorously in the comments that this paper should be judged as a contribution to the VI literature and not the SMC literature.  Unfortunately, I will recommend that this paper is rejected. It is my opinion that R4's concerns were not fully addressed.\n\nOn the one hand, I agree with the authors that there is significant value to be had in exploring variants of SMC for VI. Indeed, some prior art, like FIVO and IWAE, contributed little to the Monte Carlo literature. I believe that these were good contributions.\n\nOn the other hand, I am concerned that the current draft does not clearly circumscribe its contributions. I read the sections that disuss the works of Schmon et al. (2019) and Kudlicka et al. (2020), and the writing did not leave me with a clear enough sense of the differences. I also read the abstract and introduction of the paper. The introduction of the paper positions this work clearly within the VI literature, but does not clearly discuss prior SMC art, e.g., it does not cite Kudlicka et al. (2020). Despite citing rejection control for SMC, the writing of the abstract and introduction left me with the impression that this work was the first to introduce *unbiased, partial* rejection control for SMC. I believe that impressions matter and that the machine learning community should be generous to adjacent communities when assigning credit.\n\nI realize that my decision is a matter of taste. I also want to say that I am confident that the authors have a clear sense of where their contribution sits, and I suspect that it is a valuable contribution. However, I cannot recommend the draft in its current form. If this is a contribution to the VI literature, as the authors argue, then the authors should not hesitate to give full credit to prior SMC art. My reading of the current draft still leaves me confused about which aspects of the SMC estimator are actual contributions.", "URL": "https://openreview.net/forum?id=r7L91opmsr", "ID": "r7L91opmsr", "Venue": "ICLR-2021"}
{"Title": "Bootstrapped Meta-Learning", "Authors": ["Sebastian Flennerhag", "Yannick Schroecker", "Tom Zahavy", "Hado van Hasselt", "David Silver", "Satinder Singh"], "Abstract": "Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that metric can be used to control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent - without backpropagating through the update rule.", "ID": "b-ny3x071E5", "URL": "https://openreview.net/forum?id=b-ny3x071E5", "Review": [{"correctness": "4: All of the claims and statements are well-supported and correct.", "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.", "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.", "flag_for_ethics_review": ["NO."], "recommendation": "10: strong accept, should be highlighted at the conference", "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.", "review": "The paper proposes *Bootstrapped Meta-Learning,* a new meta-learning algorithm for hyperparameter optimization. Drawing inspiration from temporal difference learning techniques in reinforcement learning, the meta-learner is asked to predict the result of additional unrolled steps of the optimization process, by minimizing a carefully selected distance to a target generated during training. This allows for longer meta-learning optimization horizons, without the need for differentiation through longer optimization trajectories. The method is tested for hyperparameter optimization for reinforcement learning, including learning the exploration hyperparameter for a behaviour policy, and in multi-task meta-learning. **Originality:** The method proposed in the paper is, to the best of my knowledge, novel. The related work section adequately connects the algorithm with existing work in similar directions. \n\n**Significance:** The empirical results show considerable improvement w.r.t. well-performing baselines; moreover, the general idea behind the method could inspire future research.\n\n**Rigour:** Both the theoretical results and the experimental protocols seem sound and solid to me.\n\n**Strengths**\n\n- The method is based on a conceptually compelling and inspiring idea.\n- The empirical results are remarkable. The ablations and additional experiments (also from the Appendix) help in understanding what matters in the practical algorithm, as well as highlighting the important parts of the contribution.\n- In particular, being able to meta-learn hyperparameters for a behaviour policy can open up new avenues for exploration in reinforcement learning.\n- The theoretical results have a clear scope (although not so large) and provide some easy to understand local improvement guarantees.\n- The paper is well-written and generally easy to follow.\n\n**Minor Concerns / Questions**\n\n- I believe the name *matching function* makes the presentation of the method a little bit harder to digest. Since the function is a pseudometric (i.e., the larger it is, the larger the distance from the target), it should really be called with a name that reminds the reader of this nature (e.g., *mismatch function*).\n- I enjoyed the theoretical results, but it is a pity that they only deal with targets of specific forms and, especially, with $L=1$ only. Ideally, theoretical result with a dependency on $L$ would shed some light on the benefits and limitations of longer bootstrapping horizons.\n- Can the authors elaborate on the connection between the way the bootstrapping target is formed in their method and traditional temporal difference learning? In particular, the grounding role of that subtracted gradient \"nudging the trajectory in a descent direction\" is the same as the one of the reward in temporal difference learning; but, while the reward is at the beginning of the trajectory, the grounding is here at the end of the optimization subtrajectory. Is there any mathematical connection beyond the general shared motivation?\n- As briefly touched upon in some passages of the paper, when the underlying function is highly nonlinear, there is the risk that the bootstrapping mechanism can lead the optimization process in worse areas of the landscape. For instance, if the function in Figure 1 had a bump/plateaux where $\\tilde w$ is, the bootstrapping mechanism would cause more troubles than standard meta-gradients. Why is this not happening in practice?\n\n-------\n*After rebuttal*: I am happy with the answer the authors provided and the update to the paper, which will help the readers understand the relationship between the proposed method and TD-learning. Overall, the improvements make me believe that this paper should be highlighted at the conference, to give other researchers working in the field the possibility to get inspired by this new idea. I am thus raising my score to 10. The algorithmic and empirical contributions of the paper, as well as the theoretical grounding, largely justify in my opinion its acceptance at the conference. "}, {"correctness": "4: All of the claims and statements are well-supported and correct.", "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.", "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.", "flag_for_ethics_review": ["NO."], "recommendation": "8: accept, good paper", "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.", "review": "\u200c\u200cThe paper presents a new meta-learning algorithm to address two shortcomings of standard meta-optimization algorithms: curvature (the meta-learner's objective is typically constrained to the same type of geometry as the learner), and limited evaluation (the meta-objective is evaluated only with-in a K-step horizon, ignoring future learning dynamics). The proposed algorithm addresses these two issues by minimizing the distance to a bootstrapped target under a chosen metric. Empirically, the new algorithm  achieved a new state-of-the art for model-free agents on the Atari ALE benchmark and yielded gains in multi-task meta-learning. Theoretically, some guarantees on performance improvements are provided.\n **\u200cOriginality**\n\nThe paper is original and novel, proposing a new algorithm to overcome two shortcomings of standard meta-optimization algorithms: curvature mismatch and limited evaluation.\n\n**Quality**\n\nThe paper is technically sound, and claims are backed by solid experiments in the reinforcement learning and multi-task meta-learning evaluation setting.\n\n**Clarity**\n\nThe paper is clear, however, the algorithm description in section 3 is very abstract. I think the paper would benefit from a running example and a dedicated section and pseudo-code describing the algorithm and how it can be instantiated in different experimental settings.\n\n**Significance**\n\nThe work is significant and will benefit the reinforcement and meta-learning community by addressing some of the limitation of the current meta-learning algorithms.\n\n**Limitations**\n\n- The theoretical analysis is limited to noiseless 1-step target updates.\n- The experimental evaluation in the multi-task meta-learning setting is limited to only compare with MAML on computer vision applications. \n\n**Questions to Authors**\n\n- Some engineering / handcrafting is still required by the machine learning practitioner to select what \"target\" the meta-learner is going to optimize, as well as the proper \"metric\" for the meta-learner to optimize for. Could the authors comment a bit about what heuristics they used when making these decisions? and whether the automation for this process is possible or not? \n- What would it take to extend the analysis beyond 1-step noiseless target updates?\n- How does the performance of BMG compare to alternative meta-learning algorithms like R2D2, Meta-OPT-net and prototypical networks? Have the authors experimented with other meta-learning benchmarks beyond image classification?\n\n**Minor Typos**\n\n- Abstract: \"show that metric\" -> \"show that a metric\"\n \u200c\u200c\n "}, {"correctness": "4: All of the claims and statements are well-supported and correct.", "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "flag_for_ethics_review": ["NO."], "recommendation": "10: strong accept, should be highlighted at the conference", "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.", "review": "This paper broadly considers meta-learning, a.k.a. bilevel optimization, across single-task, multi-task, supervised learning, and reinforcement learning settings. The authors aim to resolve two issues with the standard outer-loop gradient-based optimization of the meta-parameters (assuming a differentiable inner-loop): first, since the meta-learning objective is typically computed from learner parameters after applying up to $K$ inner-loop updates, the meta-optimization is myopic in that it does not optimize for further inner-loop improvement after $K$ steps; second, since the functional form of the learner's objective $f$ is used to drive the outer-loop updates, the meta-learning objective inherits the curvature of $f$. The main algorithmic contribution consists of a family of meta-learning objectives called bootstrapped meta-learning, in which meta-parameters are optimized to bring post-inner-loop learner parameters $x^{(K)}$ closer to a bootstrap target (which are also learner parameters) computed from $x^{(K)}$. The authors show that bootstrapped meta-learning generalizes the \"direct\" (my terminology) gradient-based meta-parameter optimization used in many previous works, recovering it when using specific choices for the bootstrap computation function and learner parameter matching function. With certain strong assumptions, the authors theoretically motivate the use of gradient-based bootstrap target functions for bootstrapped meta-learning in terms of optimization progress. The authors make several experimental contributions: they use bootstrapped meta-learning to achieve state-of-the-art model-free performance on Atari-57, demonstrate the viability of bootstrapped meta-learning in few-shot image classification on miniImageNet, and show that the more flexible, general form of bootstrapped meta-learning can enable meta-learning parameters that do not appear in the computation graph for the task objective, e.g. meta-learning the exploration rate of the behavior policy in $\\epsilon$-greedy $Q$-learning. Strengths\n- The proposed idea is simple.\n- The writing is clear.\n- To my knowledge, the proposed idea is novel yet concretely linked to many past works by virtue of generalizing them. The authors give the precise form to recover MG from BMG.\n- The proposed idea results in strong improvement of the STACX agent in Atari-57, resulting in a state-of-the-art result (caveat: for model-free agents; the gap to model-based agents remains large).\n- The authors demonstrate that the proposed idea is suitable in few-shot image classification, a popular application of meta-learning for few-shot learning.\n- The authors run informative ablation studies that support the intuition behind the benefit of BMG: resolving curvature and mitigating myopia.\n\nWeaknesses\n- Given that you say BMG is compatible with any update function (so long as it is differentiable in the meta-parameters), it would be nice to have some experiments on learned sequence model update rules (e.g. RNN). All current experiments use update rules with a fixed functional form.\n- I am not putting much weight on section 4 (\"Performance Guarantees\") given the gap between its assumptions and results vs. what is actually implemented, and the restriction to local optimization.\n\nMinor comments\n- p. 3, target bootstrap paragraph, 3rd line: are we missing a learning rate for the expression of the target?\n- p. 5, the actor-critic RL objective and Eq. 4: as-is, we are always minimizing policy entropy; is there a sign error? I believe that the meta-learning community will find this paper interesting. It provides new insights into formulating meta-learning models and gives examples of such insights being applied to obtain empirical gains. The paper is well-written and features exemplary empirical execution.\n\nPOST-REBUTTAL: I have updated my score to 10 and confidence to 5. I think this paper should get an oral, if not best paper. It is the best in my batch and is arguably the best I have read all year. It may be the best paper I have ever peer-reviewed. "}, {"correctness": "4: All of the claims and statements are well-supported and correct.", "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.", "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.", "flag_for_ethics_review": ["NO."], "recommendation": "8: accept, good paper", "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.", "review": "The paper presents Bootstrapped meta-gradients (BMG), an extension of typical meta-gradients (MG) for the task tuning meta-parameters that control the learning process (i.e. update step) of a learner.  In general terms, MG applies a (meta-)parameterised update rule to a learner for K steps, and then backpropagates through these updates to update the meta-parameters in the direction that improves the performance of the adapted learner.  The authors identify two limitations to this approach, and propose BMG to address them. (i) MG is myopic, in the sense that it does not account for future learning dynamics beyond these K steps, therefore BMG proposes to bootstrap a target from the K-step parameters (in practice by continuing to optimise w.r.t. parameterised update rule for L-1 steps, and then taking a final step w.r.t. a fixed objective to ground the signal). (ii) MG updates are necessarily restricted to be within the geometry of the parameterised learning process.  In contrast, BMG introduces a matching function to measure the distance between the learners K-step parameters and bootstrapped target in an arbitrary (and hopefully more-suitable space).\n\nAfter framing the problem and BMG, the authors provide a discussion of the necessary conditions for BMG to guarantee performance improvements, though ultimately the presented algorithm is justified empirically using experiments on (i) a toy RL problem, (ii) the Atari RL test suite, (iii) multi-task few-shot adaptation on an image recognition task.  In all settings, BMG provides significant improvement over meta-learning baselines \u2014 most impressively achieving a new SOTA on Atari.  Moreover, key features of BMG are highlighted, including the ability to extend the meta-learning horizon without increasing the number of updates steps through which we must backpropagate, and that behavioural parameters outside of the update rule (specifically, epsilon in epsilon-greedy exploration) can also be meta-learned. **Strengths**\n\n-  The paper is generally clear and well written.  It is content dense in and certain areas would benefit from more detailed discussions.  However, the appendices are very detailed and provide answers to almost all of the questions that arise during first reading.  In my suggestions below I\u2019ve highlighted a few areas I feel this content would be especially beneficial (and one pare where the main text could be stripped-back as there is always a trade-off with fixed page limits).\n\n- The key ideas of BMG, bootstrapping a target to combat myopic MG updates and using matching functions to improve the meta-learning dynamics, are novel and well motivated.  I strongly believe that these ideas will be of interest to the broader meta-learning community and will likely lead to multiple future research directions.\n\n- The empirical results are strong and thorough.  The toy-model grid world in Sec 5.1 effectively highlights key properties of the proposed algorithm \u2014 such as the ability of BMG to exploit longer meta-learning horizons \u2014 and extensive testing on Atari environments (Sec 5.2) shows a significant improvement of STACX and leaves little doubt of the efficacy of BMG for self-tuning algorithms.   Moreover, the demonstrated performance improvement of BMG over MAML (sec 6) suggests broad possible applications and further endorses the proposed algorithm.\n\n- BMG also has a couple of nice properties besides raw performance: (i) achieving less myopic updates without having to backpropagate through many update steps of the inner-loop parameters and (ii) allowing for the meta-learning of \u201cbehavioural parameters\u201d that are not used in the learning rule (exploration epsilon is used as the example).  Whilst (i) is clearly desirable in the pursuit of computational efficiency, (ii) is very intriguing and opens the door to new applications of meta-learning (indeed, whilst it is mentioned in the abstract and summaries, I believe this point is slightly undersold in the main text and that this experiment could be presented in more detail than the single paragraph at the end of Sec 5.1 it is given \u2014 however, given the page limit I can understand the authors predicament).\n\n**Weakenesses**\n\n- My primary concern is not with the content of the paper, but that the it can be quite difficult to intuitively link the numerous experiments back to intuition or interpretation of the results.  I believe this was largely because the exact methodologies are difficult to follow and, indeed, I found it was essential to refer to the appendices repeatedly to fully unpack the experiments and appreciate the results.  Whilst I am sympathetic to space constraints, I believe the authors would be well served to provide more detailed descriptions in the main text or, ideally, an algorithm box.\n\n- I find the implementation and implications of the experiment on multi-task few-shot learning (Sec 6) unclear in the following regards:\n\t- I do not understand the intuition of why a \u201chot\u201d expert transforms more information than a \u201ccold\u201d expert, and, moreover, why BMG is able to use this to improve performance.  Could the authors clarify these points.\n\t- I feel this section in particular suffers from a lack of formal introduction of the task and methodology.  For example, the adaptation seems to be defined as a single step, whereas the appendix (D.2) notes that 10 are used during meta-testing.  Concretely, I think a formal description of the training procedure for BMG would be appropriate in the main text.\n\n- I do not find the analysis presented in Sec 4 (\u201cPerformance Guarantees\u201d) especially insightful.  My reading is that, whilst the MG update presented in Lemma 1 can guarantee local improvement, practical implementations of BMG do not.  Empirically, however, grounding the bootstrapped target with a single final step on the meta-objective is sufficient for good performance.  This conclusion is evident from the experiments themselves, and so I would have no issue if Sec 4 was in the appendix.\n\n**Errata**\n\n- Sec 3, paragraph starting \u201cTo tackle myopia\u2026\u201d: the inline equation for $\\tilde{x}$ is missing brackets around the step counters for $x^{(K+L-1)}$.\n- Sec 5.2, second last paragraph: typo - \u201cK is more sensitive curvature and the quality of data\u201d.\n- Sec 6, sub-section BMG: typo - \u201craising the temperature in the expect allows\u201d.\n- Sec 6, sub-section Setup: typo - \u201cFor 50 meta-updates and beyond..\u201d: this should be 50k meta-updates I believe. Overall I believe this paper proposed an insightful and novel approach to addressing the stated limitations of typical MG approaches.  The authors do a good job of motivating the key innovations proposed and, given the significant research interest in MG\u2019s in the past few years, it is reasonable to assume that the methodologies presented will be of broad interest.  Moreover, the experimental results are impressive and clearly demonstrate the improved performance of BMG and analyse where this comes from.  I do believe that the experimental results would be better served with more detailed descriptions of the problem settings and methodologies, however given the overall level of detail presented in the appendices I do not doubt there validity or significance.  Even so, for the stated reasons of novelty, interest and potential impact, I believe the paper is suitable for acceptance in the current form. "}], "Decision": "Accept (Oral)", "Metareview": "This paper addresses a meta-learning method which involves bilevel optimization. It is claimed that two limitations (myopia of MG and restricted consideration of geometry of search space) that most of existing methods have can be resolved by the MBG with a properly chosen pseudo-metric. The algorithm first bootstraps a target from the meta- learner, then optimizes the meta-learner by minimizing the distance to that target under a chosen pseudo-metric. The authors also establish conditions that guarantee performance improvements and show that metric can be sued to control meta-optimization. All the reviewers agree that the idea is interesting and experiments well support it. Authors did a good job in the rebuttal phase, resolving most of concerns raised by reviewers, leading that two of reviewers raised their score. While the current theoretical results are limited to a simple case where L=1$, the method is attractive for meta-learning community. All reviewers agree to champion this paper. Congratulations on a nice work.", "Venue": "ICLR-2022"}
{"Title": "Simplifying Automated Pattern Selection for Planning with Symbolic Pattern Databases", "Authors": ["Anonymous"], "Abstract": "Pattern databases (PDBs) are memory-based abstraction heuristics that are constructed prior to the planning process, which if expressed symbolically yield a very efficient representation. Recent work in the automatic generation of symbolic PDBs has established it as one of the most successful approaches for cost-optimal domain-independent planning. In this paper, we contribute two planners, both using bin-packing for its pattern selection. In the second one, we introduce a greedy selection algorithm called Partial-Gamer, which complements the heuristic given by bin-packing. We tested our approaches on the benchmarks of the last three International Planning Competitions, optimal track, getting very competitive results, with this simple and deterministic algorithm.", "Review": [{"title": "Meta-Review", "recommendation": "Accept", "confidence": "5: absolutely certain"}, {"title": "Interesting topic, but presentation is poor. Incremental work where exact contribution is not very clear.", "review": "The presented paper introduces an extension to existing pattern \ngeneration algorithms for PDB heuristics. The work is fairly \nincremental, mostly turning screws in a large space of existing \nmethods and combining them in new ways. Also, it does not become very \nclear what the concrete contribution of this paper is. How does it \ndiffer from existing methods, such as those presented in \"On Creating \nComplementary Pattern Databases\" by Franco et al. (2017)? I guess a \nseparate related work section could help a lot in clarifying the \ndifference to existing methods and the contribution of this work.\n\nThe way the paper is currently written, by interleaving background on\nexisting methods (which btw. makes more than half of the paper), with\nremarks about which ingredients of these methods are used in the \npresented approach with or without modifications, makes it hard to\nfollow. More generally, the write-up is not very good and could be\nsignificantly improved. \n\nAlso, at least in the presentation, the suggested changes are not very \nsystematic, there is no consistent thread. Rather, I have the \nimpression that various existing methods are combined to somehow \nachieve higher total coverage.\n\nStill, the experimental evaluation is not very convincing. When \nchoosing the right benchmark set (IPC'18), the new method slightly \noutperforms the shown existing methods (including Complementary2). \nWhen adding the IPC'11+14 benchmarks, Complementary2 is slightly \nahead. What happens when using all IPC benchmarks ('98-18)?\n\nNevertheless, I think the topic of the paper is interesting for the\nHSDIP audience and the presented ideas might foster interesting\ndiscussions, since automatic pattern generation for PDB heuristics\nseems to be a hot topic these day.\n\nIf the paper gets accepted, I urge the authors to work on the write-up\nand introduce a separate related work section.\n\n\n\nMinor comments:\n- abstract: \"...which*,* if expressed symbolically*,*...\"\n- next sentence: \"in the automatic* generation\"\n- please remove the copyright statements from the first page.\n- Def.1: s_o*and*s_*\n- Def.1: the sets {\\cal A} and {\\cal A}^+ are not defined\n- figure 1 floats into the margin\n- \"Franco et al... shows that ... (Franco etal).\" remove one of the \ncites.\n- end of paragraph below Def.4: \"symbolic symbolic\"\n- \"contribute to more that on*ce*\"\n- \"the correlation between the cross product [..] is rather weak.\"\nI agree that it is weaker than for explicit representations, but still\n\"rather weak\" is not a good description.\n- the cite of FD \"[Helmert, 2006]\" has a different format than the \nother cites\n- the style of writing is sometimes very informal, e.g.\n\"Baeckstroem prefers the SAS+ formalism\", or \"in the most prestigious\nand attended [..] track\"\n- for consistency, please put the caption of table 2 below the table\n- the bibliography does not seem to be in AAAI style\n- the references Edelkamp 2002a and b refer to the same paper, same\nfor Holte and Hernadvoelgyi.\n- there is a typo in the reference to Preditis 1993 *heuristics*", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"title": "The paper evaluates two methods for generating patterns for symbolic pattern databases but omits many crucial details and comparisons.", "review": "The paper certainly addresses an interesting research question and I think trying to understand what made the Complementary planners work well in IPC 2018 is a worthwhile endeavor. Unfortunately, I don't think the paper in its current form takes us closer to this goal. The paper presents two algorithms but many details are left out. This makes it impossible to understand how the algorithms work and how they differ from previous work. Furthermore, the experiment section is missing many essential comparisons, making it impossible to judge how the algorithms compare to previous work.\n\nIntroduction:\n* Each concrete plan has an abstract counterpart in an induced abstraction of the same length, not \"a shorter one\".\n* The meaning of a pattern in the sliding-tile puzzle depends on the representation. It can be a set of tiles or a set of positions.\n\nBackground:\n* No need to introduce STRIPS, PDDL and PSVN. It doesn't seem you're using these later in the paper.\n* Definition 1: there's no definition for \\phi, \\mathcal{A}, and the concept of plans.\n* \"The set of reachable states is generated on-the-fly\" -> it is defined declaratively\n* You describe \"spurious states\" in the background section, but never evaluate their impact or describe how you deal with them.\n* I think \"unreachable states\" would be a better name than \"spurious states\" if that is indeed what you mean.\n\nPattern Databases:\n* The codomain of heuristics should include \\infty.\n* \"the sum of heuristic values obtained via projection to a disjoint variable set is admissible\"\n  -> This is only true if no two operators affect the same variable.\n* \"The projection of state variables induces a projection of operators and requires 0/1 cost partitioning\"\n  -> No, any cost partitioning algorithm can be used. Or you can maximize over the estimates instead of summing them.\n* Definition 5: \"maximize the average heuristic [value]\"\n  -> why not the heuristic value of the initial state or a set of samples? I think this needs some motivation.\n\nPattern Selection and Cost Partitioning:\n* \"SCP relies on only using those costs which each heuristic uses to create an abstract plan.\"\n  -> This sounds misleading: SCP preserves *all* heuristic estimates under the given cost function.\n* \"[SCP] is time consuming compared to other cost partitioning methods (Pommerening, Helmert, and Bonet 2017)\"\n  -> the reference does not mention SCP. Seipp, Keller, Helmert (ICAPS 2017) state\n  \"[SCP] can be computed at negligible overhead during the construction of the heuristic\"\n\nGreedy Selection:\n* The way I understand it, Gamer uses *steepest-ascent* hill climbing and Gamer-style uses *simple* hill climbing. If that's the case, I think using these terms might clarify the presentation.\n\nBP-PDB:\n* \"Even though reducing the number of PDBs used to group all possible variables does not guarantee a better PDB, by having a smaller PDB collections, it is less likely to miss interactions between variables due to them being placed on different PDBs.\"\n  -> This suggests trying to base the pattern generation on the variable interactions and not on their domain sizes as bin packing does.\n* \"the method will start generating pattern collections stochastically\"\n  -> How?\n* \"We then decide whether to add a pattern collection to the list of selected patterns if it is estimated that adding such PDB will speed up search.\"\n  -> How?\n* \"To evaluate the fitness function\"\n  -> Which fitness function?\n\nGreedyPDB:\n* The limits of 50s and 75s for bin packing seem very large, given that most tasks have less than 1000 variables and the greedy bin packing algorithms FFI and FFD should run very fast.\n* The first sentence of the last paragraph in this section appears twice in the paper.\n* I guess some variable names are wrong in Algorithm 1?: SelPDBs vs. P_sel and SizeLim vs. Size\n* Why do you iteratively increase the size limit in the PACKER function? I thought either the variables fit or they don't.\n* How does Generate_P work?\n* How exactly does GreedyPDB differ from Complementary 1 and 2?\n\nExperiments:\n* Figure 2 should probably use a histogram or similar instead of a line plot.\n* Which time and memory limits do you use?\n* Tables 1 and 2 are superfluous since they are subsumed in Table 3.\n* I don't think there's an advantage in knowing the coverage score of an oracle planner.\n* What is \"advanced\" bin packing?\n* The paper states that GreedyPDB is a \"new state-of-the-art in cost-optimal planning\". However, Table 3 shows that Complementary 2 solves more tasks than GreedyPDB in 14 domains, while the opposite is true in only 6 domains.\n* How do the parameters of GreedyPDB influence the resulting heuristic?\n* I think the experiments focus too much on comparing whole planners and too little on comparing pattern selection algorithms.\n* Keeping all other parts of the planner the same, the paper should compare different approaches for selecting patterns. Table 3 goes into the right direction by evaluating the three Greedy PDB variants, but this comparison only includes new pattern selection algorithms. The comparison should also include the following pattern selection algorithms:\n  * the algorithms used in BP-PDB and Complementary 1 and 2\n  * the Gamer pattern selection (without it, we cannot judge whether \"Gamer-style\" has any merit)\n  * the hill climbing pattern generator (Haslum et al., AAAI 2007)\n  * the systematic pattern generation methods of patterns up to size 2 and 3 (Pommerening et al., IJCAI 2013), which have been shown to yield very strong heuristics.\n\nReferences:\n* Some of the references for Stefan Edelkamp are missing a venue.\n\nStyle:\n* Horizontal and vertical lines in tables only help if not all cells are fully surrounded by lines. One line below the header should be sufficient.\n* Your inline citation macro seems to put extra whitespace between the authors and the year.\n* No need to abbreviate domain names in Table 3, there's enough space.\n* The paper has numerous typos. I recommend having it proof-read.\n\nReproducibility:\n* BP-PDB only has a very high-level description and the pseudo-code for GreedyPDB is also missing some crucial details (e.g., EM algorithm). The paper does not promise to make any code or results available. Therefore, it won't be possible to replicate the results or build on them in future work.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}], "Metareview": "Dear Authors,\nthank you very much for your submission. We are happy to inform you that\nwe have decided to accept it and we look forward to your talk in the workshop.\nPlease, go over the feedback in the reviews and correct or update your papers\nin time for the camera ready date (May 24). In particular, please address the\ncomments raised by both reviewers regarding clarity, discussion of related work,\nand additional experimental results, by making use of the additional space (9 pages) \nallowed by HSDIP.\nBest regards\nHSDIP organizers", "URL": "https://openreview.net/forum?id=BJldVWPTPN", "ID": "BJldVWPTPN", "Venue": "ICAPS-HSDIP-2019"}
{"Title": "Environment Diversification with Multi-head Neural Network for Invariant Learning", "Authors": ["Bo-Wei Huang", "Keng-Te Liao", "Chang-Sheng Kao", "Shou-De Lin"], "Abstract": "Neural networks are often trained with empirical risk minimization; however, it has been shown that a shift between training and testing distributions can cause unpredictable performance degradation. On this issue, a research direction, invariant learning, has been proposed to extract causal features insensitive to the distributional changes. This work proposes EDNIL, an invariant learning framework containing a multi-head neural network to absorb data biases. We show that this framework does not require prior knowledge about environments or strong assumptions about the pre-trained model. We also reveal that the proposed algorithm has theoretical connections to recent studies discussing properties of variant and invariant features. Finally, we demonstrate that models trained with EDNIL are empirically more robust against distributional shifts. ", "Review": [{"recommendation": "Accept", "confidence": "Certain", "award": "No"}, {"rating": "5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.", "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.", "questions": "1) While the intro speaks about a shift between train and test distributions, in the end it seems you address the problem of a mix of multiple distributions in both train and test. That is the train and test are not fundamentally different, they are just both formed through a combination of multiple distributions, is this right?\n2) If so, what is the meaning of $\\mathcal{E}_{all}$ in your setup?\n3) Do I assume correctly that the output and input spaces are aligned across the environments (including the supports of the distributions)?\n4) What are $\\Phi$ and $\\Psi$? Are these some general functions such as small MLPs?\n5) In equation (5) softmax you sum across the E_learn invoronments, Does it mean that the number of environments needs to be fixed beforehand?\n6) How is the environment decided at test time, when no labels are available? Which head shall be used? ", "ethics_flag": "No", "ethics_review_area": [], "soundness": "2 fair", "presentation": "2 fair", "contribution": "2 fair", "code_of_conduct": "Yes", "review": "\nThe paper discusses the problem of shift between train and test data distributions. The authors assume that training data originate from multiple 'environments', each with particular distribution. They propose a new framework (EDNIL) through which they can infer the environments from data (without human labeling) and then train an environment invariant model. The principal assumption is that of the existence of `invariant features` - the distribution of the outputs conditioned on these is the same across all the environments. The loss is a combination of multiple terms motivated through information theoric arguments. The algorithm consists of alternating steps to infer the environments and train the invariant model. Multiple experiments are provided to corroborate the effectiveness of the method. Train/test distribution shift is an important and fully answered problem, particularly relevant for all practical applications where the standard iid assumptions typically do not hold. As such the paper addresses a significant question for the community.\nI found the paper somewhat difficult to parse and follow. Most importantly, I found the description of the whole setup (shift from train to test distributions or a mix of multiple environments in train and test) rather confusing. But I have more question marks (see Questions) indicating a fairly low clarity of the paper. \nIn the end the solution is an alternating optimization in which through the first step the environments are inferred though a form of supervised clustering (similarity metric is the model accuracy) and then this info is used to train a prediction model. If my interpretation is correct, this does not seem very novel. Societal impact - not addressed, not directly relevant.\nLimitations - invalidity of graphical model assumption is mentioned. Yes, is reasonable.  "}, {"rating": "6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.", "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.", "questions": "1. Problem 2.1 needs to be explained more, discussing the use of Shannon entropy. Also clarify the novelty; I believe Equation (3) was proposed in [1] but Equation (4) is a contribution?\n2. In Equation (4), is it enough that LHS is just > 0 while being arbitrarily small? Should the LHS be maximized instead?\n3. The proposed method in Equation (5) needs more explanation on why it was chosen to be this way. I believe it is done to ensure that data is divided into environments that maximize label prediction in the individual environments. This is interesting but requires more justification in the text. Related to my previous comment, $H(Y | X_v) - H(Y | X_v, \\mathcal{E}_\\text{learn})$ might be actually maximized with this objective, and not just >0. \n\n4. Discussing $L_{IP}$ before discussing the $M_{IL}$ model hampers clarity. It should also be clarified/reiterated that $L_{IP}$ is only used to update the $M_{EI}$ network and not $\\Phi$, and that there are two separate invariance preserving objectives (one for $M_{IL}$ and one for $M_{EI}$). \n5. It is very interesting that the number of environments (clusters) need not be specified accurately for EDNIL to work (as seen in Figure 6). More discussion regarding why this is so will be helpful, i.e., which part of the loss ensures this.\n\n\nMinor:\n\n1. Please use \\text{} in math equations for non-variables, such as \\text{learn}, \\text{LD}, etc. Also, use \\exp in Equation (5).\n2. Line 19: \u201c\u2026 aims at learning causality expected to be robust \u2026\u201d should be \u201c..aims at learning causal features expected to be robust..\u201d.\n3. Line 106: should be \u201cHRM equips a joint learning framework\u201d. \n\n[1] Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. Heterogeneous risk minimization. In International Conference on Machine Learning (ICML), 2021.", "ethics_flag": "No", "ethics_review_area": [], "soundness": "3 good", "presentation": "2 fair", "contribution": "2 fair", "code_of_conduct": "Yes", "review": "Paper proposes a new method to infer environment labels that can then be used for invariant learning (such as IRM methods). Essentially, it uses a neural network for clustering data into environments with appropriate loss functions. The method is more scalable and less sensitive to initializations than the prior methods. Experiments show significant improvements for distribution-shifted tasks without environment labels. Strengths:\n\n1. Paper tackles an important challenge of robust learning with no environment labels and solves the problems with existing works.\n\n2. Experiments in varied tasks, including images and text, show significant performance improvements. Experiments also help answer questions about the effect of initializations, effect of number of environments.\n\n\nWeaknesses:\n\n1. Clarity can be improved a lot; especially Section 2.2 and Section 3 (Methodology). As a result, the paper\u2019s contributions are muddled with existing work (for example, which of the learning objectives and loss terms are novel?). \n2. Paper can be improved a lot with better justifications for the choices made in the paper. Currently the loss functions seem a little arbitrary; for example, why are Equations (5) and (9) the best ways to achieve their respective goals. \n3. There are no theoretical guarantees provided by the prior works, but this can be future work. \n\n Yes "}, {"rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.", "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.", "questions": "* I was a little unclear on the motivation for using the environment (in)dependency objective - I can see its utility in the ablation studies, but what was the reason, conceptually speaking, to introduce this in the first place?\n* Beyond looking at the value of L_ED during training, can an experiment be formulated to test whether the inferred environments (samples from P(e|X,Y)) are indeed \u201cdiverse\u201d in the way the authors are hoping for?\n* Section 3 is a bit hard to parse given that the three objectives are introduced in several places (lines 129\u2013133, Fig 1, Eqn 6). I would suggest that the authors take care to discuss these in the same order each time and include the symbols alongside the objective names (e.g. \u201cLabel dependency objective\u201d => \u201cLabel dependency objective ($L_{LD}$\u201d in line 129). I also found it a bit confusing that some objectives are for the inference net Phi while others are for the invariant learner. While I can see that Fig 1 attempts to make this distinction by placing the loss terms in different parts of the figure, it would be even clearer if the networks (or their parameters) were used as arguments to the losses, e.g. L_ED(Phi, Psi) = L_ED(Psi) + beta L_LD(Psi) + gamma L_IP(Phi) in Eqn 6). I would also order the terms in Eqn 6 in the same order as they appear in the text.\n* Since the variance of per-env losses is used as L_IP, it is worth mentioning REx (https://arxiv.org/abs/2003.00688) as a related work, since it uses a similar objective (with known environments).\n* Regarding the tabular experiments, there are some limitations of the UCI-Adult dataset (https://openreview.net/forum?id=bYi_2708mKK) that would be relevant/interesting to NeurIPS readers/attendees (perhaps in a footnote).\n* Environment-free invariant learning is a very ambitious problem, which could be difficult to solve in some settings. I would be curious to hear about experimental settings where EDNIL fails; I think these would interest the reader and also make the limitations of this method a bit more clear.\n* When mentioning how the training/validation splits for Waterbirds are merged [lines 290\u2013291], it would be good to mention that the original validation split contains all four groups in equal proportions, whereas original training split has unequal proportions.\n* I find the double negatives needed to parse Table 1 to be slightly confusing.\n", "ethics_flag": "No", "ethics_review_area": [], "soundness": "3 good", "presentation": "2 fair", "contribution": "4 excellent", "code_of_conduct": "Yes", "review": "The authors propose a novel environment-free invariant learning method that uses an auxiliary network to learn environment-specific features, from which environment inferences can be derived. These inferred environments labels are then used to train and invariant model.\n\nUPDATE 2022-aug-05: The paper's clarity has been improved so I am increasing my score by one. I like the idea of the paper, and was impressed by the experimental results showing that the proposed method EDNIL realizes good worst-group performance in a variety of settings, while addressing the sensitivity to initialization that we see in EIIL. In my opinion, the experimental results are sufficient but the presentation should be improved (see below for specific suggestions).\n\nStrengths\n* I like the experimental results. The \u201ceffortless initialization\u201d result (Fig 7), showing that the proposed method is not sensitive to initial ERM features, which is a known issue with EIIL, is especially impressive.\n* The paper does a good job of describing related approaches to environment-free invariant learning, and comparing against these methods empirically.\nWeaknesses\n* The paper can be hard to read at times. Some parts would benefit from additional exposition. For example the idea of \u201cdiversity\u201d of inferred environments is mentioned in passing with a citation, but it is not fully explained what type of diversity is needed, and how it can be measured formally? The cited paper (https://arxiv.org/abs/2004.05007) mentions that the correlation pattern between Y and spurious features should vary across environments\u2026is the outcome we hope for when introducing L_ED?\n No [line 430]. I suggest that the authors add a discussion of possible societal impacts to a future revision. Given the connections of invariant learning to fairness (https://arxiv.org/abs/2010.07249) and the use of a fairness benchmark in the experiments, there should be plenty to discuss. "}, {"rating": "7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.", "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.", "questions": "- In general, the presence of way too many acronyms can confuse the reader. I do understand that presenting all this material in 9 pages is quite a challenge but the density is a bit excessive in some passages of the manuscript.\n- lines 74-93 seem a bit hard to follow, especially about the role of Phi and Xv.\n- the role of beta and gamma seems important, but the authors only give space in the main text to the discussion of the extreme cases beta=0 and gamma=0. Can a few comments be added about the range of optimal values for these parameters? How hard is it to fine-tune them and how variable are they across different learning problems?\n- lines 166-169 (and later figure 7 and its description): what is the intuitive explanation of the reduced dependence on the training stage for the ERM initialization. Is it about the role of the L_IP term, that avoids trivial environment split scenarios?\n- line 195 is not clear, probably just needs rewriting.\n- lines 227-230 the sentence seems to be missing a verb and becomes a bit obscure.\n- line 236: removing the coefficient beta -> in what sense? setting it to 0, or to 1? Probably rephrasing could help.\n- In general, it seems to me that this work could be very impactful also for the field of ML fairness, where the different environments could represent protected categories that are misrepresented in the available data and the learning model risks to become biased. Did the authors consider this connection?\n", "ethics_flag": "No", "ethics_review_area": [], "soundness": "4 excellent", "presentation": "3 good", "contribution": "4 excellent", "code_of_conduct": "Yes", "review": "In the present paper, the authors tackle the challenging problem of learning a supervised ML model capable of identifying and adapting to different environments hidden in the training data. The method, EDNIL, is composed of two jointly learned models, that take care of the environment identification, the learning of the invariant representations and the label predictions, produced by a multi-headed neural network. The proposed model is compared to different alternative models from the literature of the field, in different challenging benchmarks, and the results show that it closely achieves the best possible invariant learning performance. \n\nAFTER REBUTTAL\nThe authors have addressed the main concerns raised during the review. I confirm my assessment of this paper as solid and deserving of acceptance in the conference. The authors introduce a very flexible and effective learning framework, incorporating the main strengths of the previously introduced models and surpassing many of the limitations thereof. The novel method is shown to perform extremely well when compared to its competitors, achieving close to the performance of IRM with access to the oracle environments. \n\nI think the main weakness of this paper is in the density of the presentation and in the abundance of acronyms that make the read quite challenging, especially for a non-expert. I think the main limitations of the paper are well acknowledged by the authors. "}], "Decision": "Accept", "Metareview": "This work presents a novel environment-free invariant learning method that uses an auxiliary network to learn environment-specific features, from which environment inferences can be derived. The method is composed of two jointly learned models, that take care of the environment identification, the learning of the invariant representations, and the label predictions, produced by a multi-headed neural network. The proposed model is compared to different alternative models from the literature of the field, in different challenging benchmarks, and the results show that it closely achieves the best possible invariant learning performance.\n\nAfter some initial discussions, all reviewers agreed that this work is ready for publication, as the work addresses an important problem, presents good empirical results, and will be of significant interest to the community. \n", "URL": "https://openreview.net/forum?id=FDmIo6o09H", "ID": "FDmIo6o09H", "Venue": "NeurIPS-2022"}